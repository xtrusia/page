---
layout: post
title: OVN Architecture
nav_order: 2
---

# OVN Architecture

```
이 문서는 계속 작업중입니다.
``` 

OVN(오픈 가상 네트워크Open Virtual Network)는 가상 네트워크 추상화를 지원하기 위한 시스템이다. OVN은 가상 네트워크 추상화를 네이티브 지원하기 위해 현존하는 OVS의 기능들을 보완한다. 여기에는 가상 L2와 L3 오버레이, 보안 그룹이 포함된다. DHCP와 같은 서비스 또한 가능한 기능이다. OVS처럼, OVN의 설계 목표는 거대한 규모에서도 운용이 가능한 제품으로써 운영 가능한 품질의 구현을 갖는 것이다.

OVN 배포는 다양한 구성 요소로 이뤄진다.

* 클라우드 관리 시스템(CMS, Cloud Management System)은 OVN의 말단 클라이언트(사용자 및 관리자를 통한)이다. OVN 통합integration은 CMS특정 플러그인과 관련 소프트웨어(아래 확인)의 설치를 요구한다. OVN은 처음에는 오픈스택OpenStack을 CMS로 잡았다. 일반적으로 하나의 CMS를 상정하지만, 서로 다른 OVN 배포 부분을 관리하는 다중 CMS 환경도 생각해 볼 수 있다.
물리 또는 가상 노드에 설치된 OVN 데이터베이스는 중심에 위치한다.
* 하나 이상의(또는 많은 수) 하이퍼바이저. 하이퍼바이저는 Open vSwitch를 구동해야 하며, OVS 소스 트리의 [Documentation/topics/integration.rst](https://github.com/openvswitch/ovs/blob/master/Documentation/topics/integration.rst){:target="_blank"}에 서술된 인터페이스를 구현한다. Open vSwitch가 지원하는 하이퍼바이저 플랫폼은 어떤 것이든 사용 가능하다.
* 0개 이상의 게이트웨이. 게이트웨이는 터널과 물리 이더넷 포트 간 패킷 상호 포워딩을 통해 터널 기반 논리 네트워크를 물리 네트워크로 확장한다. 이는 비 가상화된 머신을 논리 네트워크에 참여할 수 있도록 한다. 게이트웨이는 물리 호스트, 가상 머신, 또는 vtep(5)기술을 지원하는 ASIC 기반 하드웨어 스위치여야 한다. 하이퍼바이저와 게이트웨이는 모두 전송 노드transport node 또는 섀시chassis라 불린다.

아래의 다이어그램은 OVN의 주 구성요소 및 관련 소프트웨어의 상호작용interact를 보여준다. 다이어그램의 최상단부터 보면, 다음과 같다.

* 위에서 정의한 클라우드 관리 시스템
* OVN/CMS 플러그인 는 OVN에 연결짓기 위한 CMS의 구성요소이다. 플러그인의 주 목적은 CMS의 환경설정 데이터베이스에 CMS 특정 포멧으로 저장된 논리 네트워크 환경설정에 대한 CMS의 개념을 OVN이 이해할 수 있는 중간 표현으로 해석하는 것이다. 이 구성요소는 필수적으로 CMS에 특정되어 있다. 따라서, 새 플로그인은 OVN에 통합된 각 CMS를 위해 개발될 필요가 있다. 아래 다이어그램에 있는 모든 구성요소는 CMS에 독립적이다.
* OVN Northbound 데이터베이스는 OVN/CMS 플러그인에 의해 전달된passed down 논리 네트워크 환경설정의 중간 표현을 수신한다. 데이터베이스 스키마는 CMS에서 사용된 개념을 impedance matched 구현한다. 따라서, 논리 스위치, 라우터, ACL 등의 개념을 직접 지원한다. 자세한 사항은 [ovn-nb(5)](https://www.ovn.org/support/dist-docs/ovn-nb.5.html){:target="_blank"}을 보라 OVN Northbound 데이터베이스는 클라이언트가 두가지이다. 상단의 OVN/CMS 플러그인과 하단의 ovn-northd이다.
* [ovn-northd(8)](https://www.ovn.org/support/dist-docs/ovn-northd.8.html){:target="_blank"} 은 상위 OVN Northbound 데이터베이스, 그리고 하위 OVN Southbound 데이터베이스에 접속한다. 이는 논리적 네트워크 환경설정을 기존conventional 네트워크 개념으로 변환한다. 이는 OVN Northbound 데이터베이스에서 취해져서, 하단의 OVN Southbound에 있는 논리 데이터패스datapath 플로우flows으로 변환된다.
* OVN Southbound 데이터베이스는 시스템의 중심이다. 이 데이터베이스의 클라이언트는 상단의 [ovn-northd(8)](https://www.ovn.org/support/dist-docs/ovn-northd.8.html){:target="_blank"}, 하단 모든 전송 노드에 있는 [ovn-controller(8)](https://www.ovn.org/support/dist-docs/ovn-controller.8.html){:target="_blank"}이다. OVN Southbound 데이터베이스는 세 종류의 데이터를 포함한다. 
  하이퍼바이저 또는 다른 노드에 닿는 방법을 지정하는 물리 네트워크(PN) 테이블, 논리 데이터패스 플로우라 불리는 논리 네트워크를 서술하는 논리 네트워크(LN), 논리 네트워크 구성요소를 물리 네트워크에 연결짓는 바인딩Binding 테이블이 그것이다. 하이퍼바이저는 PN과 Port_Binding 테이블을 구성하며, [ovn-northd(8)](https://www.ovn.org/support/dist-docs/ovn-northd.8.html){:target="_blank"}은 LN 테이블을 구성한다. 
  
  OVN Southbound 데이터베이스 성능은 여러 개의 전송 노드와 함께 규모 조정이 가능해야 한다. 이는 병목현상 때문에 [ovsdb-server(1)](https://man7.org/linux/man-pages/man1/ovsdb-server.1.html){:target="_blank"}를 대상으로 한 작업이 필요하다. 사용 가능성을 위한 클러스터링이 필요할 수 있다.

나머지 구성요소는 각 하이퍼바이저마다 존재한다.

* [ovn-controller(8)](https://www.ovn.org/support/dist-docs/ovn-controller.8.html){:target="_blank"}은 각 하이퍼바이저 및 소프트웨어 게이트웨이에 존재하는 OVN의 에이전트agent이다. Northbound는 OVN Southbound 데이터베이스에 접속하여 OVN 환경설정과 그 상태를 학습하고, 하이퍼바이저의 상태를 PN 테이블 및 바인딩 테이블의 섀시Chassis 열을 구성한다. Southbound는 네트워크 트래픽 제어를 위해 OpenFlow 컨트롤러인 [ovs-vswitchd(8)](http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.8.txt){:target="_blank"}에 접속하고, 로컬 [ovsdb-server(1)](https://man7.org/linux/man-pages/man1/ovsdb-server.1.html){:target="_blank"}에 접속하여 Open vSwitch 환경설정을 모니터링 및 제어한다.

* [ovs-vswitchd(8)](http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.8.txt){:target="_blank"}와 [ovsdb-server(1)](https://man7.org/linux/man-pages/man1/ovsdb-server.1.html){:target="_blank"}은 Open vSwitch의 기존 구성요소이다.

                                         CMS
                                          |
                                          |
                              +-----------|-----------+
                              |           |           |
                              |     OVN/CMS Plugin    |
                              |           |           |
                              |           |           |
                              |   OVN Northbound DB   |
                              |           |           |
                              |           |           |
                              |       ovn-northd      |
                              |           |           |
                              +-----------|-----------+
                                          |
                                          |
                                +-------------------+
                                | OVN Southbound DB |
                                +-------------------+
                                          |
                                          |
                       +------------------+------------------+
                       |                  |                  |
         HV 1          |                  |    HV n          |
       +---------------|---------------+  .  +---------------|---------------+
       |               |               |  .  |               |               |
       |        ovn-controller         |  .  |        ovn-controller         |
       |         |          |          |  .  |         |          |          |
       |         |          |          |     |         |          |          |
       |  ovs-vswitchd   ovsdb-server  |     |  ovs-vswitchd   ovsdb-server  |
       |                               |     |                               |
       +-------------------------------+     +-------------------------------+

# OVN에서 정보 흐름

OVN 흐름에서의 환경설정 데이터는 north에서 south로 흐른다. CMS는 그 OVN/CMS 플러그인과 northbound 데이터베이스를 통해 논리 네트워크 환경설정을 ovn-northd로 전달한다. 결국, ovn-northd는 환경설정을 저수준 형태로 변환하고, 이를 southbound 데이터베이스를 통해 모든 섀시로 전달한다.

OVN 흐름에서의 상태 정보는 south에서 north로 흐른다. OVN은 현재 몇 가지의 상태 정보만을 제공한다. 먼저, ovn-northd는 northbound Logical_Switch_Port 테이블의 up 열을 구성(populate)한다. 만약 southbound Port_Binding 테이블에 논리 포트의 chassis 열이 비어있지 않다면, up을 true로 설정한다. 그렇지 않으면 false로 설정한다. 이는 CMS가 VM의 네트워킹이 시작되었을 때 이를 탐지할 수 있도록 한다.

둘째로, OVN은 그 환경설정이 실현되었음을 CMS에 피드백한다. 즉, CMS가 제공한 환경설정이 효과를 발생할 때 마다 피드백한다. 이 기능은 CMS가 순차 번호 프로토콜sequence number protocol에 참여해야 한다. 이는 다음과 같이 동작한다.

  1. CMS가 northbound 데이터베이스에 환경설정을 갱신하면, 같은 트랜잭션의 일부로써, NB_Global 테이블의 nb_cfg 열의 값을 늘린다. (이는 CMS가 환경설정이 언제 실현되었는지 알기 원할 때에만 필요하다.) [ovn-nb.ovsschema](https://github.com/ovn-org/ovn/blob/main/ovn-nb.ovsschema){:target="_blank"} 참고

  2. ovn-northd가 northbound 데이터베이스의 주어진 스냅샷snapshot에 따라 southbound 데이터베이스를 갱신하면, 같은 트랜잭션의 일부로써 northbound NB_Global의 nb_cfg를 복사하여 southbound 데이버테이스의 SB_Global 테이블로 복사한다. (따라서, 두 데이터베이스의 옵저버 모니터링은 southbound 데이터베이스와 northbound가 서로 정보를 주고받은 시점을 알 수 있다.) [ovn-sb.ovsschema](https://github.com/ovn-org/ovn/blob/main/ovn-sb.ovsschema){:target="_blank"} 참고

  3. ovn-northd가 southbound 데이터베이스 서버로부터 그 변경이 적용됨에 대한 확인confirmation을 수신한 후에, northbound NB_Global 테이블의 sb_cfg를 내려보낸(pushed down) nb_cfg 버전으로 갱신한다. (따라서, CMS 또는 기타 옵저버는 southbound 데이터베이스에 접속 없이 southbound 데이터베이스의 정보 전달 시점을 알 수 있다. )

  4. 각 섀시의 ovn-controller 프로세스는 갱신된 nb_cfg를 포함한 갱신된 southbound 데이터베이스를 수신한다. 이 프로세스는 결국 섀시의 Open vSwitch 인스턴스에 설치된 물리 플로우flow를 갱신한다. Open vSwitch로부터 물리 플로우가 갱신되었다는 확답을 수신하면, southbound 데이터베이스의 고유(its own) Chassis 레코드를 갱신한다.

  5. ovn-northd는 southbound 데이터베이스에 있는 모든 Chassis 레코드의 nb_cfg 열을 모니터링한다. 이들 모든 레코드의 최소값을 계속 추적하며, 이를 northbound NB_Global 테이블의 hv_cfg 열에 복제한다. (따라서, CMS 또는 기타 옵저버는 언제 모든 하이퍼바이저가 northbound 환경설정을 가지는 지 알 수 있다.)

# 섀시 설정
OVN 배포에서의 각 섀시는 OVN에만 할당된 Open vSwitch 브릿지로 설정되어야 한다. 이는 통합 브릿지(integration bridge)라 불린다. 시스템 시작 스크립트는 ovn-controller를 구동 하기 전에 이 브릿지를 생성한다. 만약 이 브릿지가 ovn-controller 시작 시에 존재하지 않으면, 아래에 제안된 기본 환경설정을 가지고 자동으로 생성될 것이다. 통합 브릿지의 포트는 다음을 포함한다.

  * 모든 섀시에서, OVN이 논리 네트웍 연결을 유지하기 위해 사용하는 터널 포트. ovn-controller는 이 터널 포트를 추가, 갱신, 삭제한다.

  * 하이퍼바이저에서, 모든 VIF는 논리 네트워크에 연결(attached)된다. TUN/TAP 이나 VETH 쌍과 같은 소프트웨어로 에뮬레이션 된 포트를 통해 연결된 인스턴스의 경우, 하이퍼바이저 자체는 일반적으로 통합 브릿지에 포트를 생성하고 이들을 꽂을 것이다. 보통 하드웨어 오프로드와 사용되는 representor 포트를 통해 연결된 인스턴스의 경우, CMS의 방향성에 따라 ovn-controller는 representor 포트 룩업lookup을 위한 VIF 플러그 제공자provider를 알아보고, 이를 통합 브릿지에 꽂는다(상세 정보는 [Documentation/topics/vif-plug-providers/vif-plug-providers.rst](https://docs.ovn.org/en/stable/topics/vif-plug-providers/vif-plug-providers.html){:target="_blank"}를 참고하라.). OVN 논리 포트와 VIF간의 매핑을 위해 이 두 경우 모두 Open vSwitch 소스 트리에 있는 Documentation/topics/integration.rst에 서술된 규약을 따른다.

  * 게이트웨이에서, 시스템 시작 스크립트는 ovn-controller를 시작 하기 전에, 논리 네트워크 연결을 위한 물리 포트를 브릿지에 추가한다. 이는 더 복잡한 설정에서, 다른 브릿지로의 패치 포트(patch port)일 수 있다. 물리 포트 대신에 말이다.

다른 포트는 통합 브릿지에 연결되어서는 안된다. 특히, 기저 네트워크에 연결된 물리 포트(논리 네트워크에 연결된 물리 포트인 게이트웨이 포트와는 반대이다.)는 통합 브릿지에 연결되어서는 안된다. 대신 기저 물리 포트는 별도의 Open vSwitch 브릿지에 연결되어야 한다(사실 어떤 브릿지에도 연결될 필요는 없다.).

통합 브릿지는 다음에 서술된 바와 같이 설정되어야 한다. 아래와 같은 설정의 각 의미는 [ovs-vswitchd.conf.db(5)](https://www.openvswitch.org/support/dist-docs/ovs-vswitchd.conf.db.5.html){:target="_blank"}에 문서화되어 있다.

- fail-mode=secure
    ovn-controller 시작 전에, 독립된 논리 네트워크 사이의 패킷 전환을 피한다. 자세한 정보는 ovs-vsctl(8)의 Controller Failure Settings 부분을 참고하라.
- other-config:disable-in-band=true
    통합 브릿지를 위한 in-band 제어 플로우를 억제한다. OVN은 원격 컨트롤러 대신 지역 컨트롤러(유닉스 도메인 소켓을 통한)를 사용하기 때문에, 이는 일반적이지는 않다.(추가요망). 그러나 같은 시스템의 다른 브릿지가 in-band 원격 컨트롤러를 갖도록 하는 것이 가능하다. 그리고 이 경우, in-band 제어가 본래 설정된 플로우를 억제한다. 더 자세한 정보는 문서를 참고하라.

통합 브릿지의 관습명은 br-int이다. 그러나 다른 이름도 사용 가능하다.

# 논리 네트워크
OVN에서의 논리 네트워크 개념에는 논리 스위치와 논리 라우터가 포함된다. 이들은 각각 논리 버전의 이더넷 스위치와 IP 라우터이다. 그들의 물리 버전처럼, 논리 스위치와 라우터도 복잡한 토폴로지에 연결할 수 있다. 논리 스위치와 라우터는 보통 순수한 논리 엔티티(entity)이다. 즉, 어떤 물리적 위치에 연관되거나 구애받지 않는다. 그리고 OVN에 참여하는 각 하이퍼바이저에서 분산을 고려하여 구현되어 있다.

<u>논리 스위치 포트</u>(LSPs)는 논리 스위치에 출입하는 연결 지점이다. 논리 스위치 포트에는 여러 종류가 있다. 가장 일반적인 종류는 VM이나 컨테이너를 위한 연결 지점인 VIF를 나타내는 것이다. VIF 논리 포트는 그 VM의 물리 위치와 연관되어 있다. 이는 VM이 이주할 때 변경이 가능하다(VIF 논리 포트는 전원이 꺼져 있거나 정지된 VM과도 연결이 가능하다. 이러한 논리 포트는 위치location 및 연결이 없다.).

<u>논리 라우터 포트</u>(LRPs)는 논리 라우터에 출입하는 연결 지점이다. LRP는 논리 라우터를 논리 스위치 또는 다른 논리 라우터로 연결한다. 논리 라우터는 논리 스위치를 통해 VM, 컨테이너, 다른 네트워크 노드로 간접적으로만 연결된다.

논리 스위치와 논리 라우터는 서로 다른 종류의 논리 포트를 갖는다. 따라서, 논리 스위치 포트 또는 논리 라우터 포트 중 어떤 것인지 언급을 반드시 해야 한다. 그러나, 논리 포트는 보통 논리 스위치 포트를 의미한다.

VM이 VIF 논리 스위치 포트로 패킷을 보낼 때 Open vSwitch 플로우 테이블은 논리 스위치와 기타 논리 라우터, 그리고 만나게 될 논리 스위치를 시뮬레이션한다. 이는 물리 매체를 통해 패킷을 전송하지 않고 이뤄진다. 이 플로우 테이블은 모든 스위칭 및 라우팅 결정 및 행동을 구현한다. 만약 플로우 테이블이 궁극적으로 다른 하이퍼바이저(또는 전송 노드 종류 아무거나)에 연결된 논리 포트로 패킷을 출력하기로 했다면, 이는 물리 네트워크 전달 및 송신을 위한 패킷의 캡슐화 시점이다.

## 논리 스위치 포트 종류
OVN은 여러 종류의 논리 스위치 포트를 지원한다. 위에 언급된 VM 또는 컨테이너에 연결된 VIF 포트가 가장 일반적인 LSP 종류이다. OVN northbound 데이터베이스에서, VIF 포트는 그 **type**{:.text-green-300}에 빈 문자열을 갖는다. 이 절은 또한 추가적인 포트 종류를 서술한다.

**router** 논리 스위치 포트는 논리 스위치를 그 쌍으로 특정 LRP로 지정된 논리 라우터로 연결한다. 

**localnet** 논리 스위치 포트는 논리 스위치를 물리 VLAN으로 브릿징(bridge)한다. 논리 스위치는 하나 이상의 **localnet** 포트를 갖는다. 이러한 논리 스위치는 다음과 같은 두 시나리오로 사용된다.

  하나 이상의 **router** 논리 스위치 포트로, L3 게이트웨이 라우터와 분산 게이트웨이를 물리 네트워크에 연결하는 경우.
  
  하나 이상의 VIF 논리 스위치 포트로, VM이나 컨테이너를 물리 네트워크에 직접 연결하는 경우. 이 경우, 논리 스위치는 실제로는 논리는 아니다. 이것이 물리 네트워크로부터 insulated된 것이 아니라 물리 네트워크에 브릿지되어 있어 독립적일 수 없고, IP 주소 네임스페이스에 중첩(overlapping)되기 때문이다. 배포는 OVN 제어 플레인plane의 장점을 얻기 위한 환경설정이나 포트 보안 및 ACL같은 기능을 선택할 수 있다.

논리 스위치가 다수의 **localnet** 포트를 포함한다면, 다음을 추측할 수 있다.

  각 섀시는 **localnet** 물리 네트워크 중 하나를 위한 브릿지 매핑만 갖는다.

  서로 다른 물리 네트워크 연결을 갖는 서로 다른 섀시에 위치한 스위치의 VIF 포트간 상호 연결성을 이루기 위해, fabric은 이러한 인접 물리 네트워크 세그먼트 간 L3 라우팅을 구현한다.

노트: 위의 내용은 섀시가 서로 다른 스위치에 속한 이상 여러 물리 네트워크에 연결(plugged)될 수 없음을 말한다.

**localport** 논리 스위치 포트는 VIF 논리 스위치 포트의 특수한 종류이다. 이들 포트는 특정 부분에 묶이지 않고 모든 섀시에 존재한다. 이들 포트로의 트래픽은 터널을 통해 절대 포워딩되지 않을 것이다. 그리고 이들 포트로부터의 트래픽은 오직 같은 섀시로 이동되는 것으로 여겨진다. 보통 수신된 요청의 응답의 경우이다. 메타데이터 프록시 프로세스는 모든 호스트의 이 포트에 연결된다. 그리고 같은 네트워크 내의 모든 VM은 터널을 통한 트래픽 전송 없이도 같은 IP/MAC 주소를 사용해서 여기에 도달할 것이다. 더 자세한 정보는 OpenStack의 networking-ovn 문서를 보라.

LSP 타입인 vtep과 l2gateway는 게이트웨이를 위해 사용된다. 더 자세한 정보는 아래 **게이트웨이**를 보라.

## 구현 상세
이 개념은 OVN이 내부적으로 구현된 상세이다. 이들은 사용자와 관리자의 흥미를 여전히 끌고 있다.

<u>논리 데이터패스</u>는 OVN southbound 데이터베이스에서 논리 네트워크의 상세 구현이다. ovn-northd는 northbound 데이터베이스에 있는 각 논리 스위치 또는 라우터를 southbound 데이터베이스 Datapath_Binding 테이블의 논리 데이터패스로 변환한다.

대부분의 경우, **ovn-northd**는 또한 OVN northbound 데이터베이스에 있는 각 논리 스위치 포트를 southbound 데이터베이스의 **Port_Binding** 테이블의 레코드로 변환한다. 후자의 테이블은 northbound **Logical_Switch_Port** 테이블과 대략 묶인다. 이는 다양한 논리 포트 바인딩(binding)을 갖는다. 많은 종류가 northbound LSP 종류와 직접 연관된다. LSP 종류는 VIF(빈 문자열), **localhost**, **localport**, **vtep**, **l2gateway**를 포함해서 위와 같이 다뤄진다.

Port_Binding 테이블은 논리 스위치 포트 종류와 직접 관련되지 않는 몇몇 포트 바인딩 종류를 갖는다. 일반적인 것은 <u>논리 패치 포트</u>로 알려진 **patch** 포트 바인딩이다. 이들 포트 바인딩은 쌍으로 생긴다. 그리고 어느쪽이든 진입하는 패킷은 다른쪽으로 나온다. **ovn-northd**는 논리 패치 포트를 사용해서 논리 스위치 및 논리 라우터에 함께 접속한다.

**vtep**, **l2gateway**, **l3gateway**, **chassisredirect**와 같은 포트 바인딩은 게이트웨이에서 사용된다. 이들은 아래 **게이트웨이**에서 설명한다.

# 게이트웨이
게이트웨이는 논리 네트워크와 물리 네트워크 사이에 제한된 연결을 제공한다. 또한 서로 다른 OVN 배포간 연결성도 제공한다. 이 절에서는 전자에 집중하고, 후자는 **OVN 배포 상호연결**에서 자세히 다룬다.

OVN은 다양한 종류의 게이트웨이를 지원한다.

<u>VTEP 게이트웨이</u>
VTEP 게이트웨이는 OVN 논리 네트워크를 Open vSwitch에 수반되는 OVSDB VTEP 스키마를 구현하는 물리(또는 가상) 스위치에 연결한다('VTEP 게이트웨이' 용어는 약간 잘못된 이름이다. VTP이 단지 VXLAN 터널 종단점이기 때문이다. 그러나 잘 정립된 이름이다.). 더 자세한 정보는 **VIF의 수명 주기**를 보라.

VTEP 게이트웨이의 주 사용 사례는 OVSDB VTEP 스키마를 지원하는 물리 ToR 스위치를 사용해서, 물리 서버를 OVN 논리 네트워크에 연결하는 것이다.

<u>L2 게이트웨이</u>
L2 게이트웨이는 단순히 특정 섀시에서 사용가능한 지정된 물리 L2 세그먼트를 논리 네트워크에 연결한다. 물리 네트워크는 사실상 논리 네트워크의 일부가 된다.

L2 게이트웨이를 설정하기 위해, CMS는 **l2gateway** LSP를 적절한 논리 스위치에 추가하고, 바인드되어야 하는 섀시의 이름을 LSP 옵션에 설정한다(확인요망). **ovn-northd**는 이 환경설정을 southbound **Port_Binding**에 복사한다. 지정된 섀시에서, **ovn-controller**는 이 세그먼트를 향하거나 오는 패킷을 적절히 포워딩한다.

L2 게이트웨이 포트는 **localnet** 포트와 같은 기능을 갖는다. 그러나, **localnet** 포트의 경우, 물리 네트워크가 하이퍼바이저 간 전송자가 된다. L2 게이트웨이의 경우, 패킷은 여전히 터널을 통해 하이퍼바이저 간 전달된다. **l2gateway** 포트는 물리 네트워크에 있는 패킷을 위해서만 사용된다. L2 게이트웨이를 위한 애플리케이션은 VTEP 게이트웨이와 비슷하다. 예를 들어, 비 가상화 머신을 논리 네트워크에 추가하는 것이 그것이다. 그러나 L2 게이트웨이는 ToR 하드웨어 스위치에서 특별히 지원을 필요로 하지 않는다.

<u>L3 게이트웨이 라우터</u>
**논리 네트워크** 에서 설명하겠지만, 보통의 OVN 논리 라우터는 분산형이다. 이들은 한 곳에만 구현되지 않고, 모든 하이퍼바이저 섀시에 존재한다. 이는 중앙집중적으로 구현되어야 하는 SNAT과 DNAT같은 스테이트풀(stateful) 서비스에는 문제가 된다.

이러한 기능을 허용하기 위해, OVN은 L3 게이트웨이 라우터를 지원한다. 이는 OVN 논리 라우터로, 지정 섀시에 구현이 된다. 게이트웨이 라우터는 보통 분산 논리 라우터와 물리 네트워크 간에 사용된다. VM과 컨테이너가 연결되는 이면에 존재하는 분산된 논리 라우터와 논리 스위치는 사실 모든 하이퍼바이저에 존재한다. 분산된 라우터와 게이트웨이 라우터는 다른 논리 스위치에 의해 연결된다. 때로 이는 join 논리 스위치로 불린다(OVN 논리 라우터는 스위치의 간섭 없이 다른 곳에 직접 연결될 수 있다. 그러나, OVN 구현은 논리 스위치에 연결된 게이트웨이 논리 라우터만 지원한다. join 논리 스위치를 사용하는 것은 또한 분산 라우터가 필요로 하는 IP 주소의 갯수를 줄일 수 있다.). 반면에, 게이트웨이 라우터는 물리 네트워크에 연결되는 **localnet** 포트를 갖는 다른 논리 스위치에 연결된다.

아래의 다이어그램은 전형적인 상황을 보여준다. 하나 이상의 논리 스위치 LS1, ..., LSn은 분산 논리 라우터 LR1에 연결된다. LR1은 다시 LSJoin을 통해 게이트웨이 논리 라우터 GLR에 연결한다. 이는 또한 논리 스위치 LSlocal에 연결된다. LSlocal은 물리 네트워크로 연결하기 위한 **localnet** 포트를 포함한다.

                                       LSlocal
                                          |
                                         GLR
                                          |
                                       LSjoin
                                          |
                                         LR1
                                          |
                                     +----+----+
                                     |    |    |
                                    LS1  ...  LSn

L3 게이트웨이 라우터를 설정하기 위해, CMS는 라우터의 northbound **Logical_Router**의 **options:chassis**를 섀시의 이름으로 설정한다. 이에 응하여, **ovn-northd**는 논리 라우터를 그 이웃에 연결하기 위해 southbound 데이터베이스의 특정 **l3gateway** 포트 바인딩(**patch** 바인딩 대신)을 사용한다. 결국, **ovn-controller**은 지역에서 처리하는 대신 패킷을 이 포트 바인딩으로부터 지정된 지정 L3 게이트웨이 섀시로 터널링(tunnel)한다.

DNAT과 SNAT 규칙은 1:다 SNAT(IP 마스커레이딩)을 처리할 수 있는 중앙 위치를 제공하는 게이트웨이 라우터와 연관이 있을 수 있다. 아래에 설명된 분산된 게이트웨이 포트 또한 NAT를 지원한다.

<u>분산된 게이트웨이 포트</u>
<u>분산된 게이트웨이 포트</u> 는 <u>게이트웨이 섀시</u> 라 불리는 별도의 섀시 하나를 지정(중앙 처리를 위함)하기 위해 특별히 설정되는 논리 라우터 포트이다. 분산된 게이트웨이 포트는 외부로 연결되는 LSP를 갖는 논리 스위치에 연결해야 한다. 즉, **localnet** LSP 또는 다른 OVN 배포(**OVN 배포 상호연결**을 보라) 중 둘 중 하나에 연결된다. 분산된 게이트웨이 포트를 돌아다니는 패킷은 가능하면 게이트웨이 섀시의 참여 없이 처리된다. 그러나 그 참여가 필요한 경우, 추가적인 홉(hop)을 취한다.

아래의 다이어그램은 분산된 게이트웨이 포트의 사용을 그린다. 논리 스위치 LS1, ..., LSn은 분산 논리 라우터 LR1에 접속한다. 그리고 이는 분산 게이트웨이 포트를 통해 물리적 네트워크와 연결하기 위해 **localnet** 포트를 포함하는 논리 스위치 LSlocal에 연결된다.

                                       LSlocal
                                          |
                                         LR1
                                          |
                                     +----+----+
                                     |    |    |
                                    LS1  ...  LSn

**ovn-northd**는 한 개가 아닌 두 southbound **Port_Binding** 레코드를 생성하여 분산된 게이트웨이 포트를 표현한다. 이 중 하나는 which is used for as much traffic as it can LRP를 위해 명명된 **patch** 포트 바인딩이다. 다른 하나는 **chassisredirect** 형태를 갖는 **cr**-<u>port</u>명명된 포트 바인딩이다. **chassisredirect** 포트 바인딩은 특별한 임무를 갖는다. 패킷이 이 포트로 출력되면, 플로우 테이블은 게이트웨이 섀시로 터널링되도록 한다. at which point 이는 자동으로 **patch** 포트 바인딩으로의 출력이 된다. 게다가, 플로우 테이블은 이 포트 바인딩으로의 출력이 될 수 있다. 게이트 섀시에서 특정 작업이 일어나야 하는 경우에 말이다. 반면에 **chassisredirect** 포트 바인딩은 사용되지 않는다(예를 들어, 패킷을 전혀 받지 않는다.).

CMS는 세가지 방법을 통해 분산 게이트웨이 포트를 설정할 수 있다. [ovn-nb(5)](https://www.ovn.org/support/dist-docs/ovn-nb.5.html){:target="_blank"}의 **Logical_Router_Port**를 위한 문서 내에 **분산 게이트웨이 포트**를 확인하라.

분산 게이트웨이 포트는 고사용성을 지원한다. 하나 이상의 섀시가 지정되면, OVN은 게이트웨이 섀시로 한번에 하나만 사용한다. OVN은 BFD를 사용해서 게이트웨이 연결성을 모니터링한다. 온라인인 더 높은 우선순위를 갖는 게이트웨이를 선호한다.

논리 라우터는 여러개의 분산된 게이트웨이 포트를 가질 수 있다. 각각은 서로 다른 외부 네트워크에 연결된다. 그러나, NAT와 로드 밸런서 같은 특정 기능은 하나 이상의 분산 게이트웨이 포트 설정을 갖는 논리 라우터를 아직 지원하지 않는다.

<u>물리 VLAN MTU 이슈</u>
아래와 같은 다이어그램을 고려해 보자.

                                       LSlocal
                                          |
                                         LR1
                                          |
                                     +----+----+
                                     |    |    |
                                    LS1  ...  LSn

각 논리 스위치 LS1, ..., LSn이 LSlocal의 **localnet** 포트에 연결된 물리 VLAN 태그된 네트워크에 브릿지되어 있다고 가정하자. LR1의 분산 게이트웨이 포트를 통해서 말이다. 만약 LS<u>i</u> 로 향하는 패킷이 외부 네트워크로 향한다면, OVN은 이를 터널을 통해 게이트웨이 섀시로 전송한다. 이 패킷은 LR1의 논리 라우터 파이프라인을 이동하며(아마도 undergoes NAT), 결국에는 LSlocal의 **localnet** 포트로 향한다. 네트워크의 모든 물리 링크가 같은 MTU를 갖는다면, 이들 패킷의 터널을 통한 전송은 MTU 문제를 일으킨다. 터널의 오버헤드는 터널을 통해서부터 게이트웨이 섀시로 향하는 모든 물리 MTU를 사용하는 패킷을 막는다(파편화 없이).

OVN은 이 문제에 대해 두가지 해결책을 제시한다. **reside-on-redirect-chassis**와 **redirect-type** 옵션이 그것이다. 두 해결책 모두 각 논리 스위치 LS1, ..., LSn이 **localnet** 논리 스위치 포트 LN1, ..., LNn을 각각 포함해야 한다. 즉, 각 섀시에 존재한다는 것이다. 둘 다 패킷을 터널 대신 **localnet** 포트를 통해 전송하게 한다. They differ  in which packets-some or all-are sent this way. 이 두 옵션의 가장 알려진 트레이드오프는 **reside-on-redirect-chassis**는 환경설정이 쉽고, **redirect-type**은 east-west 트래픽의 경우 성능이 더 낫다는 것이다.

첫 해결책은 논리 라우터 포트를 위한 **reside-on-redirect-chassis** 옵션이다. 이 옵션을 LS1에서 LR1까지 설정하면, 게이트웨이 섀ㅖ시를 제외한 곳에서 LS1에서 LR1로의 포워딩을 비활성화 한다. 게이트웨이 섀시를 제외한 다른 셰시에서 이 변경이 의미하는 바는 패킷이 LN1으로 포워딩되는 대신 LR1으로 포워딩된다는 것이다. 게이트웨이 섀시의 LN1 인스턴스는 패킷을 받고 LR1로 포워딩한다. 패킷은 LR1 논리 라우터 파이프라인을 이동하며(아마도 undergoes NAT), 결국에는 LSlocal의 **localnet** 포트로 향한다. 이 패킷은 MTU 이슈를 피하기 위해 터널을 지나지 않는다.

이 옵션은 중앙화된 '분산된' 논리 라우터 LR1을 야기한다. 게이트웨이 섀시를 제외한 다른 섀시에서 LS1에서 LR1으로 포워딩되는 패킷이 없기 때문이다. 그러므로, north-south뿐만 아니라 east-west 트래픽도 게이트웨이 섀시를 통해 전달된다(The naive  ``fix’’  of  allowing east-west traffic to flow directly between chassis over LN1 does not work because routing sets the Ethernet source address  to  LR1’s  source  address. Seeing this single Ethernet source address originate from all of the chassis  will  confuse the  physical switch.)

분산 게이트웨이 포트에는 **reside-on-redirect-chassis** 옵션을 설정하지 말라. 위의 다이어그램에서, LS1, ..., LSn에서 LR1을 연결하는 LRP에 설정되어야 한다.

두번째 해결첵은 분산 게이트웨이 포트를 위한 **redirect-type** 옵션이다. 이 옵션을 **bridged**에 설정하면, 게이트웨이 섀시로 리다이랙트된 패킷이 터널되는 대신 **localnet** 포트를 통해 가도록 한다. 이 옵션은 OVN이 게이트 섀시로 리다이랙트되지 않은 패킷을 다루는 방식에는 변경을 주지 않는다.

**redirect-type** 옵션은 관리자나 CMS가 Open vSwitch 데이터베이스의 **ovn-chassis-mac-mappings**를 설정함으로써 논리 라우터를 위한 고유의 이더넷 주소를 갖는 각 참여 섀시를 설정하도록 한다. 이는 **reside-on-redirect-chassis** 의 설정보다 더 어렵다.

분산 게이트웨이 포트의 **redirect-type** 옵션을 설정하자.

<u>확장성을 위한 분산 게이트웨이 포트의 사용</u>

분산 게이트웨이 포트의 가장 주요한 목적은 외부 네트워크와의 연결성 제공이지만, 확장성을 위한 특수한 사용 사례도 존재한다.

**ovn-kubernetes**를 사용하는것 과 같은 어떤 배포에서는, 논리 스위치는 개별 섀시에 묶인다. 그리고 분산 논리 라우터에 연결된다. 이러한 배포에서, 섀시 수준 논리 스위치는 분산보다는 섀시에 중앙화된다. 즉, 각 섀시의 **ovn-controller**는 플로우 및 다른 섀시의 논리 스위치 포트를 처리할 필요가 없다. 그러나, 아무 힌트가 없다면 **ovn-controller**는 완전한 분산을 이룬 것 처럼 논리 스위치를 처리할 것이다. 이 경우, 분산 게이트웨이 포트는 매우 유용하다. 섀시 수준 논리 스위치는 분산 게이트웨이 포트를 사용하여 분산 라우터에 접속된다. 게이트웨이 섀시(또는 단일 섀시만 있는 HA 섀시 그룹)를 각 논리 스위치가 묶인 섀시로 설정함으로써 말이다. **ovn-controller**는 모든 다른 섀시의 논리 스위치의 처리를 무시할 것이다. 이는 특히 섀시가 많은 경우 확장성에 발전을 가져온다.

# VIF의 수명 주기
독립되어 표현된 테이블(Table)과 그 스키마는 이해하기가 어렵다. 아래는 그 예이다.

하이퍼바이저의 VIF는 하이퍼바이저에서 직접 구동중인 VM 또는 컨테이너에 연결된 가상 네트워크 인터페이스이다(이는 VM 내에서 구동되는 컨테이너의 인터페이스와는 다르다.).

이 예제의 단계는 OVN과 OVN Northbound 데이터베이스 스키마의 상세를 참조한다. 이 데이터베이스의 전체 내용은 [ovn-sb(5)](https://www.ovn.org/support/dist-docs/ovn-sb.5.html)와 [ovn-nb(5)](https://www.ovn.org/support/dist-docs/ovn-nb.5.html)를 각각 살펴보라.

1. VIF의 수명 주기는 CMS 관리자가 CMS 사용자 인터페이스나 API를 이용해서 새 VIF를 생성하고, 이를 스위치(OVN를 통해 논리 스위치로 구현된 것.)에 추가할 떄 시작된다. CMS는 그 고유의 환경설정을 갱신한다. 여기에는 해당 VIF와 관련된 고유의, 영구적인 식별자 <u>vif-id</u>와 이더넷 주소 <u>mac</u>이 포함된다.

2. CMS 플러그인은 새 VIF를 포함시키기 위해 **Logical_Switch_Port** 테이블에 행을 추가함으로써 OVN Northbound 데이터베이스를 갱신한다. 새 행에는, **name**은 <u>vif-id</u>를, **mac**은 <u>mac</u>을 뜻하며, **switch**는 OVN 논리 스위치의 **Logical_Switch** 레코드를 가리킨다. 또한 다른 열들도 적절히 초기화된다.

3. **ovn-northd**는 OVN Northbound 데이터베이스 갱신을 수신한다. 그 후 대응되는 갱신을 OVN Southbound 데이터베이스에 수행한다. 새 포트를 반영하기 위해 OVN Southbound 데이터베이스 **Logical_Flow** 테이블을 추가함으로써 말이다. 예를 들어, 새 포트의 MAC 주소로 향하는 패킷을 인지하기 위핸 플로우(flow) 추가 및 새 포트를 포함하는 브로드캐스트 및 멀티캐스트 패킷을 전달하기 위한 플로우 갱신이 그것이다. 또한 **Binding** 테이블의 레코드를 생성하고, **chassis**를 나타내는 열을 제외한 모든 열을 populate한다.

4. 모든 하이퍼바이저에서, **ovn-controller**는 이전 단계에서 **ovn-northd**가 수행한 **Logical_Flow** 테이블의 갱신을 수신한다. 해당 VIF를 소유하는 VM이 전원이 꺼져 있는 한, **ovn-controller**는 많은 일을 수행할 수 없다. 예를 들어, VIF가 실제로는 어디에도 존재하지 않기 떄문에, 해당 VIF에서 패킷을 보내거나 수신하는 것을 할당할 수 없다.

5. 언젠가 사용자는 해당 VIF를 갖는 VM의 전원을 켤 것이다. 켜진 VM이 존재하는 하이퍼바이저에서, 하이퍼바이저와 Open vSwitch([Documentation/topics/integration.rst](https://github.com/openvswitch/ovs/blob/master/Documentation/topics/integration.rst){:target="_blank"}에 서술됨) 사이의 통합은 해당 VIF를 OVN 통합 브릿지에 추가하고, 해당 인터페이스가 새 VIF의 인스턴스화라는 것을 가리키기 위해 <u>vif-id</u>를 **external_ids:iface-id**에 저장한다(이 코드는 OVN에서 새로 구현된 것이 아니다. 이는 OVS를 지원하는 하이퍼바이저가 이미 수행하고 있는 통합 작업이다.).

6. 켜진 VM이 있는 하이퍼바이저에서, **ovn-controller**는 새 인터페이스의 **external_ids:iface-id**를 인지한다. OVN Southbound DB에서, 이는 **external_ids:iface-id**로부터의 논리 포트를 하이퍼바이저에 연결하는 열인 **Binding** 테이블의 **chassis** 열을 갱신한다. 그 후, **ovn-controller**는 지역 하이퍼바이저의 OpenFlow 테이블을 갱신하여 해당 VIF를 출입하는 패킷을 적절히 처리하게 한다.

7. OpenStack을 포함하는 몇몇 CMS 시스템에서는 네트워크가 완전히 준비된 경우에만 VM을 구동한다. 이를 지원하기 위해, **ovn-northd**는 **Binding** 테이블에 있는 열을 위해 갱신된 **chassis** 열을 인지하고, 해당 VIF가 이제 사용 가능하다는 것을 가리키기 위해 OVN Northbound 데이터베이스의 **Logical_Switch_Port** 테이블의 **up** 열을 갱신하도록 한다. 이 기능을 사용하는 CMS는 VM의 실행을 허용한다.

8. VIF가 상주하고 있지 않은 모든 하이퍼바이저에서, **ovn-controller**는 **Binding** 테이블에 poulated된 열을 인지한다. 이는 **ovn-controller**에 논리 포트의 물리 위치를 제공한다. 따라서, 각 인스턴스는 그 스위치(OVN DB **Logical_Flow** 테이블의 논리 데이터패스 플로우를 기반으로)의 OpenFlow 테이블을 갱신하여 해당 VIF에 출입하는 패킷은 터널을 통해 적절히 처리된다.

9. 끝으로, 해당 VIF를 갖는 VM의 전원을 끊을 것이다. 꺼진 VM이 있는 하이퍼바이저에서, VIF는 OVN 통합 브릿지에서 삭제된다.

10. 전원이 꺼진 VM을 갖는 하이퍼바이저에서, **ovn-controller**는 VIF가 삭제되었음을 인지한다. 따라서, **Binding** 테이블의 논리 포트를 나타내는 **Chassis** 열의 내용을 삭제한다.

11. 모든 하이퍼바이저에서, **ovn-controller**는 **Binding** 테이블의 논리 포트를 위한 행에서 빈 **Chassis** 열을 인지한다. 이는 **ovn-controller**가 더 이상 해당 논리 포트의 물리적 위치를 알지 못한다는 것이다. 따라서, 각 인스턴스는 이를 반영하기 위해 그 OpenFlow 테이블을 갱신한다.

12. 결국 VIF(또는 그 VM전체)가 더 이상 누구에게도 필요하지 않은 경우, 관리자는 CMS 사용자 인터페이스 또는 API를 사용해서 VIF를 삭제한다. CMS는 그 환경설정을 갱신한다.

13. CMS 플러그인은 OVN Northbound 데이터베이스에서 VIF를 삭제한다. **Logical_Switch_Port** 테이블의 열을 삭제함으로써 말이다.

14. **ovn-northd**는 OVN Northbound 갱신을 수신하고, OVN Southbound 데이터베이스를 그에 맞게 갱신한다. OVN Southbound 데이터베이스에서 삭제된 VIF와 관련된 **Logical_Flow** 테이블과 **Binding** 테이블에서 열을 삭제 또는 갱신한다.

15. 모든 하이퍼바이저에서, **ovn-controller**는 이전 단계에서 **ovn-northd**가 수행한 **Logical_Flow** 테이블의 갱신을 수신한다. **ovn-controller**는 해당 갱신을 반영하기 위해 OpenFlow 테이블을 갱신한다. 할 수 있는게 많진 않지만, 이전 단계에서 **Binding** 테이블로부터 삭제가 되었을 때 이미 해당 VIF는 도달할 수 없게 되었을 것이다.

# VM 내의 컨테이너 인터페이스의 수명 주기
OVN은 각 하이퍼바이저에서 OVN_NB 데이터베이스에 작성된 정보를 OpenFlow 플로우로 전환하는 방식으로 가상 네트워크 추상화를 제공한다. 다중 테넌트(tenant)를 위한 가상 네트워크 보안은 OVN 컨트롤러가 Open vSwitch에서 플로우를 수정할 수 있는 유일한 요소인 경우에만 제공된다. Open vSwitch 통합 브릿지가 하이퍼바이저에 상주할 때, VM 내에서 동작하는 테넌트 작업은 Open vSwitch 플로우에는 어떠한 변경도 가할 수 없다고 생각하는 것이 합리적이다.

인프라스트럭처로부터 컨테이너에 있는 애플리케이션이 Open vSwitch 플로우를 고장내거나 수정하지 않는다는 신뢰를 제공한다면, 컨테이너는 하이퍼바이저에서 실행이 가능하다. 이는 또한, 컨테이너가 VM 내부에서 실행되고 OVN 컨트롤러가 추가한 플로우를 갖는 Open vSwitch 통합 브릿지가 같은 VM에 있을 때에도 해당된다. 두 경우 모두, 작업흐름은 이전 절("VIF의 생명주기")에서 설명한 것과 같다.

이 절에서는 컨테이너가 VM내에서 생성되고, Open vSwitch 통합 브릿지가 하이퍼바이저 내에 상주하는 경우 컨테이너 인터페이스(CIF)의 생에 주기에 대해 다룬다. 이 경우, 컨테이너 애플리케이션이 오동작한다 하더라도, 다른 테넌트는 영향 받지 않을 것이다. 왜냐하면, VM 내에 구동중인 컨테이너는 Open vSwitch 통합 브릿지의 플로우를 수정할 수 없기 때문이다.

VM 내에 여러 컨테이너가 생성되면, 대응되는 여러 CIF가 생긴다. 이들 CIF에 관련된 네트워크 트래픽은 OVN이 가상 네트워크 추상화를 지원하기 위해 하이퍼바이저에 실행중인 Open vSwitch 통합 브릿지에 도달해야 한다. OVN은 또한 서로 다른 CIF에서 도달하는 네트워크 트래픽을 구분할 수 있어야 한다. CIF의 네트워크 트래픽을 구분짓는데는 두 가지 방법이 존재한다.

하나는 모든 CIF에 VIF를 제공하는 것이다(1:1 모델). 이는 하이퍼바이저에 많은 네트워크 장치가 존재할 수 있다는 것을 뜻한다. 모든 CIF의 관리를 위해 필요한 추가적인 CPU 주기가 필요하므로 OVS를 느리게할 수 있다. 또한, VM 내에 컨테이너를 생성 요소들 또한 하이퍼버이저의 대응되는 VIF를 생성할 수 있어야 한다는 것을 말한다.

둘째는 모든 CIF에 하나의 VIF를 제공하는 것이다(1:다 모델). OVN은 모든 패킷에 쓰여진 태그(tag)를 통해 서로 다른 CIF로 부터 오는 네트워크 트래픽을 구분할 수 있다. OVN은 이 기재를 사용하고, 태그 기재를 위해서는 VLAN을 사용한다.

1. CIF의 수명 주기는 VM 내에 컨테이너가 생성됨으로써 시작된다. 이는 VM을 생성한 같은 CMS로부터 또는 VM을 생성한 CMS와는 다른 컨테이너 Orchestration 시스템에 의해서 일 수 있다. 그것이 무엇이든, 컨테이너 인터페이스의 네트워크 트래픽이 향할 것이라 생각되는 VM의 네트워크 인터페이스에 대응되는 vif-id를 알아야 한다. 컨테이너 인터페이스를 생성하는 요소 또한 VM 내의 미사용 VLAN을 선택할 수 있어야 한다.

2. 컨테이너를 생성한 요소()는 OVN Northbound 데이터베이스를 갱신하여, 새 CIF를 포함하도록 해야 한다. Logical_Switch_port 테이블에 행을 추가한다. 새 행에는, 고유 식별자로써 name을, CIF의 네트워크 트래픽이 지나쳐야 할 VM의 vif-id로써 parent_name을, CIF의 네트워크 트래픽을 식별하기 위한 VLAN 태그로써 tag를 갖는다.

3. ovn-northd는 OVN Northbound 데이터베이스 갱신을 수신한다. 다음으로, OVN Southbound 데이터베이스에 대응되는 바를 갱신한다. OVN Southbound 데이터베이스의 Logical_Flow 테이블에 새 포트를 반영하는 행을 추가한다. 또한 Binding 테이블에 새 행을 생성하고 chassis를 식별하는 열을 제외한 모든 열을 populate한다.

4. 모든 하이퍼바이저에서, ov-controller는 Binding 테이블의 변경을 주시(subscribe)하고 있다. Binding 테이블의 parent_port 열에 값을 포함하는 새 행이 ovn-northd로부터 생성되면, external_ids:iface-id의 vif-id 값과 같은 값을 갖는 OVN 통합 브릿지를 갖는 하이퍼바이저의 ovn-controller는 지역 하이퍼바이저의 OpenFlow 테이블 값을 갱신한다. 따라서, VIF를 출입하는 특정 VLAN tag가 달린 패킷은 적절히 처리된다. 그후, 물리 위치를 반영하기 위해 Binding의 chassis 열을 갱신한다.

5. 기저 네트워크가 준비되고 나면 컨테이너 내부에는 하나의 애플리케이션만 시작 가능하다. 이를 지원하기 위해, ovn-northd는 Binding 테이블의 chassis 열의 갱신을 인지하고, CIF가 이제 활성화되었다는 것을 가리키기 위해 OVN Northbound 데이터베이스의 Logical_Switch_Port 테이블의 up 열을 갱신한다. 컨테이너 애플리케이션을 시작할 책임이 있는 요소는 해당 값을 질의한 후 애플리케이션을 시작한다.

6. 종국에는, 컨테이너를 생성하고 시작한 요소가 이들을 종료한다. 이 요소는 CMD(또는 직접)를 통해 Logical_Switch_Port 테이블의 행을 삭제한다.

7. ovn-northd는 OVN Northbound 갱신을 수신하고, OVN Southbound 데이터베이스를 그에 따라 갱신한다. 삭제된 CIF와 관련된 OVN Southbound 데이터베이스의 Logical_Flow 테이블의 행을 제거 또는 갱신한다. 또한 CIF를 위한 Binding 테이블의 열을 삭제한다.

8. 모든 하이퍼바이저에서, ovn-controller는 이전 단계에서 ovn-northd가 수행한 Logical_Flow 테이블의 갱신을 수신한다. ovn-controller는 해당 갱신을 반영하기 위해 OpenFlow 테이블을 갱신한다.

# 패킷의 구조적 물리적 수명 주기

이 절에서는 패킷이 OVN을 통해 하나의 가상 머신 또는 컨테이너에서 다른 곳으로 여행하는 과정을 서술한다. 이 부분에서는 패키의 물리적 처리에 집중한다. 패킷의 논리적 수명 주기의 서술의 경우 ovn-sb(5)의 Logical_Flow 테이블을 참조하라.

이 절에서는 아래와 같이 요약할 수 있는 여러 데이터 및 메타데이터 필드를 언급한다.

  - 터널 키(tunnel key)

    OVN이 Geneve 또는 기타 터널을 통해 패킷을 캡슐화하면, 수신하는 OVN 인스턴스가 그를 적절히 처리할 수 있도록 하기 위해 추가 데이터를 붙인다. 이는 특정 캡슐화에 따라 서로 다른 형태를 가지지만, 이 터널 키 가 가리키는 각 경우에 따르면 된다. 자세한 사항은 아래의 터널 캡슐화(Tunnel Encapsulations)를 보라.

  - 논리 데이터패스 필드

    이 필드는 처리되어야 할 패킷이 통과 하는 논리적 데이터패스를 나타낸다. OVN은 OpenFlow 1.1+에서 간단히 metadata(헷갈린다)라 부르는 이 필드를 논리 데이터패스를 저장하기 위해 사용한다(이 필드는 터널 키의 일부로 터널에 걸쳐 전달된다.).

  - 논리 입력 포트 필드
    
    이 필드는 논리 데이터패스에 진입한 패킷이 어디에서 온 것인지를 가리키는 논리 포트를 나타낸다. OVN은 Open vSwitch 확장 레지스터 번호 14(확인요망)에 이를 저장한다.

    Geneve와 STT 터널은 이 필드를 터널 키의 일부로 전달한다. 비록 VXLAN 터널은 명시적으로 논리 입력 포트를 전달하지는 않지만, OVN은 단일 논리 포트만으로 구성된 OVN의 관점으로 VXLAN을 사용하여 게이트웨이와 통신한다. 따라서 OVN은 논리 입력 포트 필드를 이 값으로 설정할 수 있다. (OVN can set the logical input port
                     field to this one on ingress to the OVN logical
                     pipeline.))

  - 논리 출력 포트 필드

    이 필드는 논리 데이터패스를 떠나는 패킷이 어느 논리 포트로 나갈지를 가리킨다. 이는 논리 인입 파이프라인의 시작에는 0으로 초기화된다. OVN은 Open vSwitch 확장 레지스터 번호 15에 이를 저장한다.

    Geneve와 STT 터널은 이 필드를 터널 키의 일부로 전달한다. VXLAN 터널은 이 논리 출력 포트 필드를 전송하지 않는다. VXLAN 터널이 터널 키에 논리 출력 포트 필드를 전달하지 않기 때문에, 패킷이 OVN 하이퍼바이저에 의해 VXLAN 터널로부터 수신되면, 패킷은 출력 포트를 결정하기 위해 테이블 8로 재전송된다. 패킷이 테이블 32에 도달하면, 이 패킷에서 MLF_RCV_FROM_VXLAN 플래그를 점검하여 지역 전송을 위해 테이블 33으로 재전송된다. 이 플래그는 VXLAN 터널에서 패킷이 도착할 때 설정된다.

  - 논리 포트를 위한 conntrack 존 필드

    이 필드는 논리 포트를 위한 연결 추적 존을 나타낸다. 이 값은 지역적인 의미만 가지고 있고, 섀시 간에는 의미가 없다. 이 값은 논리 인입 파이프라인의 시작에 0으로 초기화된다. OVN은 이를 Open vSwitch 확장 레지스터 번호 13에 저장한다.

  - 라우터를 위한 conntrack 존 필드

    이 필드는 라우터를 위한 연결 추적 존을 나타낸다. 이 값은 지역적인 의미만 가지고 있고, 섀시 간에는 의미가 없다. OVN은 DNAT을 위한 존 정보를 Open vSwitch 확장 레지스터 번호 11에 저장하고, SNat를 위한 존 정보는 Open vSwitch 확장 레지스터 번호 12에 저장한다.

  - 논리 플로우 플래그 (추후확인)

    논리 플래그는 테이블 간 컨텍스트 유지를 다루기 위한 것이다. 어떤 규칙을 뒤따르는 테이블에 매치하도록 할 건지 결정하기 위해서 말이다. OVN은 논리 플래그를 Open vSwitch 확장 레지스터 번호 10에 저장한다.

  - VLAN ID

    VLAN ID는 OVN과 VM에 있는 컨테이너 간 인터페이스로써 사용된다(더 자세한 정보는 VM 내의 컨테이너 인터페이스의 수명 주기를 보라.).

초기에, 인입 하이퍼바이저의 VM 또는 컨테이너는 OVN 통합 브릿지에 연결된 포트로 패킷을 보낸다. 따라서,

  1. OpenFlow table 0은 물리-가상 전환을 수행한다. 이는 패킷의 진입 포트를 match시킨다. 그 액션(action)은 패킷에 논리 메타데이터라는 주석을 단다. 패킷이 지나다니는 논리 데이터패스 식별을 위한 논리 데이터패스 필드 및 진입 포트를 식별하기 위한 논리 입력 포트 필드를 설정함으로써 말이다. 그러고 나서, 논리 진입 파이프라인에 들어가기 위해 테이블 8로 재전송한다.
  
  VM내의 중첩 컨테이너로부터 기인하는 패킷은 조금 다른 방식으로 다뤄진다. 해당 컨테이너는 VIF 특정 VLAN ID를 기반으로 구분할 수 있다. 따라서, 물리-논리 변환 플로우는 추가적으로 VLAN ID와 match되며, 액션(action)은 VLAN 헤더를 제거한다. 이 단계 다음에는, OVN은 컨테이너에서 온 패킷을 다른 패킷과 마찬가지로 다룬다.
  
  테이블 0은 또한 다른 섀시로부터 도달하는 패킷을 처리한다. 이는 진입 포트(즉 터널)에서 오는 다른 패킷과 구분한다. OVN 파이프라인에 막 들어오는 패킷의 경우, 액션은 이러한 패킷에 논리 데이터패스와 논리 진입 포트 메타데이터를 붙인다(annotate). 추가로, 액션은 논리 출력 포트 필드를 설정한다. 이는 논리 출력 포트가 알려진 이후에 OVN 터널링이 발생하기 때문에 사용 가능하다. 이러한 정보의 세 조각은 터널 캡슐화 메타데이터로부터 얻을 수 있다(인코딩 상세는 터널 캡슐화를 보라.). 그러고 나서, 액션은 논리 진출 파이프라인에 들어가기 위해 테이블 33에 재전송한다.

  2. OpenFlow 테이블 8에서 31은 OVN Southband 데이터베이스의 Logical_Flow 테이블로부터 논리 진입 파이프라인을 실행한다. 이 테이블은 논리 포트 및 논리 데이터패스와 같은 논리적인 개념으로 모두 표현된다. ovn-controller의 작업 중 큰 부분은 이를 대응되는 OpenFlow로 변환하는 것이다(특히 이는 표 번호를 변환한다. Logical_Flow 테이블 0에서 23은 OpenFlow 테이블 8에서 31로 변환된다.).
  
  각 논리 플로우는 하나 이상의 OpenFlow 흐름에 매핑된다. 실제 패킷은 보통 이 중 하나에만 매치(match)된다. 비록 어떤 경우에는 하나 이상의 흐름에 매칭이 가능하지만 말이다(이것이 큰 문제는 아닌것이, 이들 모두는 같은 액션을 갖기 때문이다.). ovn-controller는 논리 플로의 UUID의 첫 32비트를 OpenFlow 흐름 또는 흐름들을 위한 쿠키(cookie)로 사용한다(이들은 고유할 필요는 없다. 논리 흐름의 UUID의 첫 32비트는 고유할 필요가 없다.).
  
  몇몇 논리 플로우는 Open vSwitch으 ㅣconjuctive match 확장(ovs-fields(7)을 보라.)에 매핑될 수 있다. conjuction 액션을 갖는 플로우는 OpenFlow 쿠키 0을 사용한다. 이들이 다수의 논리 흐름과 대응되기 때문이다. 공동(conjuctive) 매칭을 위한 OpenFlow 흐름에는 conj_id의 매칭도 포함된다.
  
  몇몇 논리 플로우는 주어진 하이퍼바이저의 OpenFlow 테이블에 표현되지 않을 수 있다. 만약 하이퍼바이저에서 이들이 사용되지 않는다면 말이다. 예를 들어, 논리 스위치에 있는 VIF가 주어진 하이퍼바이저에 없고, 논리 스위치가 해당 하이퍼바이저에서 접근이 불가능하다면(예, over a series of hops through logical switches and routers starting from a VIF on the hypervisor), 해당 논리 플로우는 그곳에 표현되지 않을 것이다.
  
  대부분의 OVN 액션은 OpenFlow에 제법 명확한 구현을 갖는다. 예를 들어 next는 resubmit으로 구현되었으며, field = constant는 set_field로 구현되었다. 몇가지가 더 있는데, 다음을 보자.

  - output:
    테이블 32로 재전송되는 패킷에 의해 구현된다. 만약 파이프라인이 하나 이상의 output 액션을 수행한다면, 각각은 독립적으로 테이블 32로 재전송된다. 이는 다중 포트로 다수의 패킷 복사본을 전송하는데 사용될 수 있다( 만약 패킷이 output 액션에서 수정되지 않았고, 복사본 중 일부가 같은 하이퍼바이저로 간다면, 논리 멀티캐스트 출력 포트를 사용하는 것이 하이퍼바이저 간 대역폭을 아끼는 길일 것이다.).

  - get_arp(P, A):
  - get_nd(P, A):
    
    인자(arguments)를 OpenFlow 필드에 저장하는 걸로 구현되었다. 그러고 나서, 테이블 66으로 재전송된다. 이 테이블은 ovn-controller가 OVN Southband 데이터베이스에 있는 MAC_Binding 테이블로 부터 생성된 플로우를 populate 한다. 이들이 테이블 66에 매칭되면, 그 액션은 이더넷 목적지 주소 필드에 연결된 MAC을 저장한다.

    (OpenFlow 액션은 인자를 위해 사용된 OpenFlow 필드를 저장하고 복구(restore)한다. 따라서 OVN 액션은 이러한 일시적인 사용을 인지할 필요가 없다.)

  - put_arp(P, A, E);
  - put_nd(P, A, E);
    
    인자를 OpenFlow 필드에 저장하기 위해 구현되었다. 그러고 나서 패킷을 ovn-controller로 보낸다. ovn-controller는 MAC_Binding 테이블을 갱신한다.

    (OpenFlow 액션은 인자를 위해 사용된 OpenFlow 필드를 저장하고 복구(restore)한다. 따라서 OVN 액션은 이러한 일시적인 사용을 인지할 필요가 없다.)

  3. OpenFlow 테이블 32부터 47은 논리 진입 파이프라인에서 output 액션을 구현한다. 특히 테이블 32는 원격 하이퍼바이저로의 패킷을 처리, 테이블 33은 지역 하이퍼바이저로의 패킷을 처리하며, 테이블 34는 논리 진입 및 진출 포트가 같아서 버려져야 하는 패킷이 있는지 점검한다.

  논리 패치 포트는 특별한 경우이다. 논리 패치 포트는 물리적 위치를 갖지 않으며, 모든 하이퍼바이저에 사실상 상주한다. 따라서, 지역 하이퍼바이저의 포트로의 출력을 위한 플로우를 테이블 33은 자연히 유니캐스트 논리 패치 포트로의 출력 또한 구현한다. 그러나, 논리 멀티캐스트 그룹의 일부인 논리 패치 포트로 같은 로직을 적용하는 것은, 패킷 복제를 야기한다. 멀티캐스트 그룹에서 논리 포트를 갖는 각 하이퍼바이저 또한 논리 패치 포트로 패킷을 출력할 것이기 때문이다. 따라서, 멀티캐스트 그룹은 테이블 32에 논리 패치 포트로의 출력을 구현한다.

  테이블 32의 각 플로우는 원격 하이퍼바이저에 논리 포트를 포함하는 유니캐스트 또는 멀티캐스트 논리 포트를 위해 논리 출력 포트를 매칭한다. 각 플로우의 액션은 매칭된 포트로의 패킷 전송을 구현한다. 원격 하이퍼바이저의 유니캐스트 논리 출력 포트의 경우, 액션은 터널 키를 올바를 값으로 설정한 후, 올바를 하이퍼바이저로 향하는 터널 포트로 패킷을 전송한다(원격 하이퍼바이저가 패킷을 받으면, 테이블 0은 터널링된 패킷을 인지할 것이고, 이를 테이블 33으로 보낼 것이다.). 멀티캐스트 논리 출력 포트의 경우, 액션은 패킷의 한 복사본을 각 원격 하이퍼바이저로 보낸다. 유니캐스트 목적지의 경우와 같은 방식으로 말이다. 멀티캐스트 그룹에 지역 하이퍼바이저 포트(들)이 포함된 경우, 그 액션은 테이블 33으로 재전송한다. 테이블 32는 또한 다음을 포함한다.
  
  * MLF_RCV_FROM_VXLAN 플래그를 기반으로, VXLAN 터널로 부터 수신된 패킷을 매칭하기 위한 높은 우선순위의 규칙. 그리고 이들 패킷을 지역 전달을 위해 테이블 33으로 재전송된다. VXLAN 터널로 부터 수신된 패킷은 여기에 도달한다. 터널 키에 논리 출력 포트 필드가 없기 때문이다. 그리고 이들 패킷은 출력 포트를 결정하기 위해 테이블 8로 재전송되어야 한다.
    
  * 논리 입력 포트를 기반으로 하는 localport 형태의 포트로부터 수신된 패킷을 매칭하기 위한 높은 우선순위 규칙. 그리고 이들 패킷은 지역 전달을 위해 테이블 33으로 재전송된다. localport 형태의 포트는 모든 하이퍼바이저에 존재하고, 정의에 따라 그들의 트래픽은 터널을 통해 나가서는 안된다.

  * MLF_LOCAL_ONLY 논리 플로우 플래그가 설정되었고 그 목적지가 멀티캐스트 주소인 패킷을 매칭하기 위한 높은 우선순위 규칙. 이 플래그는 패킷이 원격 하이퍼바이저로는 전달되어서는 안된다는 것을 가리킨다. 멀티캐스트 목적지에 원격 하이퍼바이저의 포트가 포함되어 있다고 하더라도 말이다. 이 플래그는 ovn-controller가 멀티캐스트 패킷의 전송자일 경우에만 사용된다. 각 ovn-controller 인스턴스가 이들 패킷의 전송자이기 때문에, 해당 패킷은 지역 포트로만 전달될 필요가 있다.

  * 매칭이 되지 않은 경우 테이블 33으로 재전송되는 폴백(fallback) 플로우

  테이블 33에 있는 플로우는 테이블 32에 있는 것들과 유사하지만, 원격 보다는 지역적으로 존재하는 논리 포트를 위한 것이다. 지역 하이퍼바이저의 유니캐스트 논리 출력 포트의 경우, 액션은 단지 테이블 34로 재전송할 뿐이다. 지역 하이퍼바이저의 하나 이상의 논리 포트를 포함하는 멀티캐스트 출력 포트의 경우, 논리 포트 P와 같은 각각을 위해 액션은 논리 출력 포트를 P로 변경한다. 그러고 나서 테이블 34로 재전송한다.

  localnet 포트가 데이터패스에 존재할 때, 원격 포트가 localnet 포트로 스위칭(switching)을 통해 연결되는 특별한 경우가 있다. 이 경우, 원격 포트에 도달하기 위해 플로우를 테이블 32로 추가하는 대신, 논리 출력포트를 localnet 포트로 스위칭하기 위해 플로우를 테이블 33에 추가한다. 그러고 나서 마치 지역 하이퍼바이저에 논리 포트로 유니캐스트된 것 처럼 테이블 33으로 재전송한다.

  테이블 34는 논리 입력 및 출력 포트가 같고 MLF_ALLOW_LOOPBACK 플래그가 설정되지 않은 패킷을 매칭하고 버린다. 이는 다른 패킷을 테이블 40에 재전송한다.

  4. OpenFlow 테이블 40부터 63은 OVN Southbound 데이터베이스의 Logical_Flow 테이블로부터 논리 진출 파이프라인을 실행한다. 진출 파이프라인은 패킷이 전달되기 전에 마지막 검증 절차를 수행한다. 끝으로, 이는 ovn-controller가 테이블 64로 재전송함으로써 구현된 output 액션을 수행할 것이다. output을 절대 수행하지 않는 파이프라인을 위한 패킷은 사실 버려진다(물리 네트워크의 터널을 통해 재전송되었다 해도 말이다.).

  진출 파이프라인은 논리 출력 포트를 변경할 수 없고, 추가적인 터널링을 일으키지 않는다.

  5. 테이블 64는 MLF_ALLOW_LOOPBACK이 설정된 경우 OpenFlow 루프백을 우회한다. 논리 루프백은 테이블 34에서 처리되지만, OpenFlow는 기본적으로 OpenFlow 진입 포트로의 루프백을 막는다. 게다가, MLF_ALLOW_LOOPBACK이 설정되면, OpenFlow 테이블 64는 OpenFlow 진입 포트를 저장하고, 이를 0으로 설정한 후, 논리-물리 변환을 위해 테이블 65로 재전송한다. 그러고 나서, OpenFlow 진입 포트를 복구하고, OpenFlow 루프백 금지를 비활성화 한다. MLF_ALLOW_LOOPBACK이 설정 해제되면, 테이블 64 플로우는 단순히 테이블 65로 재전송된다.

  6. OpenFlow 테이블 65는 논리-물리 변환을 수행한다. 이는 테이블 0의 정반대이다. 이는 패킷의 논리 진출 포트를 매칭한다. 그 액션은 논리 포트를 표현하는 OVN 통합 브릿지로 연결된 포트로 패킷을 출력한다. 만약 논리 진출 포트가 VM내의 중첩된 컨테이너라면, 패킷을 전송하기 전에, 액션은 적절한 VLAN ID를 포함하는 VLAN 헤더를 얹는다.
   
# 논리 라우터 및 논리 패치 포트
보통 논리 라우터와 논리 패치 포트는 물리 위치를 갖지 않고, 사실 모든 하이퍼바이저에 상주한다. 이는 VM(및 VIF)가 연결된 논리 라우터와 그 논리 라우터 이면의 논리 스위치 사이의 논리 패치 포트에 해당된다.

서로 다른 서브넷에 존재하는 한 가상 머신 또는 컨테이너에서 다른 VM 또는 컨테이너로 전송된 패킷을 생각해 보자. 패킷은 이전 절(패킷의 구조적 물리적 수명 주기)에서 설명한 테이블 0에서 65로 이동할 것이다. 송산자가 연결된 논리 스위치를 나타내는 논리 데이터패스를 통해 말이다. 테이블 37에서, 해당 패킷은 같은 하이퍼바이저의 테이블 38로 재전송하는 폴백(fallback) 플로우를 사용할 것이다. 이 경우, 테이블 0에서 테이블 65의 모든 절차는 송신자가 있는 하이퍼바이저에서 발생한다.

패킷이 테이블 65에 도달하면, 논리 진입 포트는 논리 패치 포트이다. ovn-controller는 진입 파이프라인의 첫 OpenFlow 플로우 테이블 에 직접 복제 및 재전송하고, 논리 패치 포트 쌍으로 논리 진입 포트를 설정하고, 논리 패치 포트 쌍의 논리 데이터패스(이는 논리 라우터를 나타낸다.)를 사용함으로써 논리 패치로의 출력을 구현한다.

테이블 8에서 65을 다시 거치기 위해 진입 파이프라인에 재진입하는 패킷은 이번에는 논리 라우터를 나타내는 논리 데이터패스를 사용한다. 이 절차는 이전 절(패킷의 구조적 물리적 수명 주기)에서 설명한 대로 진행된다. 패킷이 테이블 65에 도달하면, 논리 진출 포트는 한 번 더 논리 패치 포트에 있을 것이다. 위에 설명한 방식 대로, 이 논리 패치 포트는 OpenFlow 테이블 8에서 65까지 패킷 재전송을 야기할 것이다. 이번에는 목적지 VM 또는 컨테이너에 연결된 논리 스위치를 나타내는 논리 데이터패스를 사용한다.

해당 패킷은 세번째이자 마지막으로 테이블 8에서 65까지 거친다. 목적지 VM 또는 컨테이너가 원격 하이퍼바이저에 존재한다면, 테이블 37은 송신자의 하이버파이저에서 원격 하이퍼바이저로 터널 포트을 통해 패킷을 전송한다. 끝으로, 테이블 65는 목적지 VM 또는 컨테이너에 직접 패킷을 출력할 것이다.

다음에 나오는 절은 두 예외를 서술한다. 이들은 논리 라우터/논리 패치 포트가 물리 위치에 연결된 경우이다.

## 게이트웨이 라우터

게이트웨이 라우터는 물리 위치에 묶인 논리 라우터이다. 여기에는 논리 라우터의 모든 논리 패치 포트를 포함한 논리 스위치의 논리 패치 포트 쌍 모두가 포함되어 있다. OVN Southbound  데이터베이스에서, 이 논리 패치 포트를 위한 Port_Binding 엔트리는 l3gateway 타입을 patch 대신 사용한다. 섀시에 묶인 이들 논리 패치 포트를 구별하기 위해서 말이다.

하이퍼바이저가 논리 스위치를 나타내는 논리 데이터패스에서 패킷을 처리하고, 논리 진출 포트가 게이트웨이 라우터에 연결을 나타내는 l3gateway 포트라면, 패킷은 게이트웨이 라우터가 상주하는 섀시로 터널 포트를 통해 패킷을 전달하는 테이블 37에 플로우를 매칭시킬 것이다. 테이블 37의 이러한 절차는 VIF에서와 같은 방식으로 이뤄진다.

## 분산 게이트웨이 포트





# 논리 라우터에 연결된 다중 로컬넷(localnet) 논리 스위치

# VTEP 게이트웨이의 수명 주기

# 외부 논리 포트를 위한 네이티브 OVN 서비스