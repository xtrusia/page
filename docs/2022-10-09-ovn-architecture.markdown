---
layout: post
title: OVN Architecture
nav_order: 2
---

# OVN Architecture

```
이 문서는 계속 작업중입니다.
``` 

OVN(오픈 가상 네트워크Open Virtual Network)는 가상 네트워크 추상화를 지원하기 위한 시스템이다. OVN은 가상 네트워크 추상화를 네이티브 지원하기 위해 현존하는 OVS의 기능들을 보완한다. 여기에는 가상 L2와 L3 오버레이, 보안 그룹이 포함된다. DHCP와 같은 서비스 또한 가능한 기능이다. OVS처럼, OVN의 설계 목표는 거대한 규모에서도 운용이 가능한 제품으로써 운영 가능한 품질의 구현을 갖는 것이다.

OVN 배포는 다양한 구성 요소로 이뤄진다.

* 클라우드 관리 시스템(CMS, Cloud Management System)은 OVN의 말단 클라이언트(사용자 및 관리자를 통한)이다. OVN 통합integration은 CMS특정 플러그인과 관련 소프트웨어(아래 확인)의 설치를 요구한다. OVN은 처음에는 오픈스택OpenStack을 CMS로 잡았다. 일반적으로 하나의 CMS를 상정하지만, 서로 다른 OVN 배포 부분을 관리하는 다중 CMS 환경도 생각해 볼 수 있다.
물리 또는 가상 노드에 설치된 OVN 데이터베이스는 중심에 위치한다.
* 하나 이상의(또는 많은 수) 하이퍼바이저. 하이퍼바이저는 Open vSwitch를 구동해야 하며, OVS 소스 트리의 [IntegrationGuide.rst](https://github.com/openvswitch/ovs/blob/master/Documentation/topics/integration.rst){:target="_blank"}에 서술된 인터페이스를 구현한다. Open vSwitch가 지원하는 하이퍼바이저 플랫폼은 어떤 것이든 사용 가능하다.
* 0개 이상의 게이트웨이. 게이트웨이는 터널과 물리 이더넷 포트 간 패킷 상호 포워딩을 통해 터널 기반 논리 네트워크를 물리 네트워크로 확장한다. 이는 비 가상화된 머신을 논리 네트워크에 참여할 수 있도록 한다. 게이트웨이는 물리 호스트, 가상 머신, 또는 vtep(5)기술을 지원하는 ASIC 기반 하드웨어 스위치여야 한다. 하이퍼바이저와 게이트웨이는 모두 전송 노드transport node 또는 섀시chassis라 불린다.

아래의 다이어그램은 OVN의 주 구성요소 및 관련 소프트웨어의 상호작용interact를 보여준다. 다이어그램의 최상단부터 보면, 다음과 같다.

* 위에서 정의한 클라우드 관리 시스템
* OVN/CMS 플러그인 는 OVN에 연결짓기 위한 CMS의 구성요소이다. 플러그인의 주 목적은 CMS의 환경설정 데이터베이스에 CMS 특정 포멧으로 저장된 논리 네트워크 환경설정에 대한 CMS의 개념을 OVN이 이해할 수 있는 중간 표현으로 해석하는 것이다. 이 구성요소는 필수적으로 CMS에 특정되어 있다. 따라서, 새 플로그인은 OVN에 통합된 각 CMS를 위해 개발될 필요가 있다. 아래 다이어그램에 있는 모든 구성요소는 CMS에 독립적이다.
* OVN Northbound 데이터베이스는 OVN/CMS 플러그인에 의해 전달된passed down 논리 네트워크 환경설정의 중간 표현을 수신한다. 데이터베이스 스키마는 CMS에서 사용된 개념을 impedance matched 구현한다. 따라서, 논리 스위치, 라우터, ACL 등의 개념을 직접 지원한다. 자세한 사항은 [ovn-nb(5)](https://man7.org/linux/man-pages/man5/ovn-nb.5.html){:target="_blank"}을 보라 OVN Northbound 데이터베이스는 클라이언트가 두가지이다. 상단의 OVN/CMS 플러그인과 하단의 ovn-northd이다.
* [ovn-northd(8)](https://man7.org/linux/man-pages/man8/ovn-northd.8.html){:target="_blank"} 은 상위 OVN Northbound 데이터베이스, 그리고 하위 OVN Southbound 데이터베이스에 접속한다. 이는 논리적 네트워크 환경설정을 기존conventional 네트워크 개념으로 변환한다. 이는 OVN Northbound 데이터베이스에서 취해져서, 하단의 OVN Southbound에 있는 논리 데이터패스datapath 플로우flows으로 변환된다.
* OVN Southbound 데이터베이스는 시스템의 중심이다. 이 데이터베이스의 클라이언트는 상단의 [ovn-northd(8)](https://man7.org/linux/man-pages/man8/ovn-northd.8.html){:target="_blank"}, 하단 모든 전송 노드에 있는 ovn-controller(8)이다. OVN Southbound 데이터베이스는 세 종류의 데이터를 포함한다. 
  하이퍼바이저 또는 다른 노드에 닿는 방법을 지정하는 물리 네트워크(PN) 테이블, 논리 데이터패스 플로우라 불리는 논리 네트워크를 서술하는 논리 네트워크(LN), 논리 네트워크 구성요소를 물리 네트워크에 연결짓는 바인딩Binding 테이블이 그것이다. 하이퍼바이저는 PN과 Port_Binding 테이블을 구성하며, [ovn-northd(8)](https://man7.org/linux/man-pages/man8/ovn-northd.8.html){:target="_blank"}은 LN 테이블을 구성한다. 
  
  OVN Southbound 데이터베이스 성능은 여러 개의 전송 노드와 함께 규모 조정이 가능해야 한다. 이는 병목현상 때문에 ovsdb-server(1)를 대상으로 한 작업이 필요하다. 사용 가능성을 위한 클러스터링이 필요할 수 있다.

나머지 구성요소는 각 하이퍼바이저마다 존재한다.

* ovn-controller(8)은 각 하이퍼바이저 및 소프트웨어 게이트웨이에 존재하는 OVN의 에이전트agent이다. Northbound는 OVN Southbound 데이터베이스에 접속하여 OVN 환경설정과 그 상태를 학습하고, 하이퍼바이저의 상태를 PN 테이블 및 바인딩 테이블의 섀시Chassis 컬럼을 구성한다. Southbound는 네트워크 트래픽 제어를 위해 OpenFlow 컨트롤러인 ovs-vswitchd(8)에 접속하고, 로컬 ovsdb-server(1)에 접속하여 Open vSwitch 환경설정을 모니터링 및 제어한다.

* ovs-vswitchd(8)와 ovsdb-server(1)은 Open vSwitch의 기존 구성요소이다.

                                         CMS
                                          |
                                          |
                              +-----------|-----------+
                              |           |           |
                              |     OVN/CMS Plugin    |
                              |           |           |
                              |           |           |
                              |   OVN Northbound DB   |
                              |           |           |
                              |           |           |
                              |       ovn-northd      |
                              |           |           |
                              +-----------|-----------+
                                          |
                                          |
                                +-------------------+
                                | OVN Southbound DB |
                                +-------------------+
                                          |
                                          |
                       +------------------+------------------+
                       |                  |                  |
         HV 1          |                  |    HV n          |
       +---------------|---------------+  .  +---------------|---------------+
       |               |               |  .  |               |               |
       |        ovn-controller         |  .  |        ovn-controller         |
       |         |          |          |  .  |         |          |          |
       |         |          |          |     |         |          |          |
       |  ovs-vswitchd   ovsdb-server  |     |  ovs-vswitchd   ovsdb-server  |
       |                               |     |                               |
       +-------------------------------+     +-------------------------------+

# OVN에서 정보 흐름

OVN 흐름에서의 환경설정 데이터는 north에서 south로 흐른다. CMS는 그 OVN/CMS 플러그인과 northbound 데이터베이스를 통해 논리 네트워크 환경설정을 ovn-northd로 전달한다. 결국, ovn-northd는 환경설정을 저수준 형태로 변환하고, 이를 southbound 데이터베이스를 통해 모든 섀시로 전달한다.

OVN 흐름에서의 상태 정보는 south에서 north로 흐른다. OVN은 현재 몇 가지의 상태 정보만을 제공한다. 먼저, ovn-northd는 northbound Logical_Switch_Port 테이블의 up 컬럼을 구성한다. 만약 southbound Port_Binding 테이블에 논리 포트의 chassis 컬럼이 비어있다면, up을 true로 설정한다. 그렇지 않으면 false로 설정한다. 이는 CMS가 VM의 네트워킹이 시작되었을 때 이를 탐지할 수 있도록 한다.

둘째로, OVN은 그 환경설정이 실현되었음을 CMS에 피드백한다. 즉, CMS가 제공한 환경설정이 효과를 발생할 때 마다 피드백한다. 이 기능은 CMS가 순차 번호 프로토콜sequence number protocol에 참여해야 한다. 이는 다음과 같이 동작한다.

1. CMS가 northbound 데이터베이스에 환경설정을 갱신하면, 같은 트랜잭션의 일부로써, NB_Global 테이블의 nb_cfg 컬럼의 값을 늘린다. (이는 CMS가 환경설정이 언제 실현되었는지 알기 원할 때에만 필요하다.)

2. ovn-northd가 northbound 데이터베이스의 주어진 스냅샷snapshot에 따라 southbound 데이터베이스를 갱신하면, 같은 트랜잭션의 일부로써 northbound NB_Global의 nb_cfg를 복사하여 southbound 데이버테이스의 SB_Global 테이블로 복사한다. (따라서, 두 데이터베이스의 옵저버 모니터링은 southbound 데이터베이스와 northbound가 서로 정보를 주고받은 시점을 알 수 있다.)

3. ovn-northd가 southbound 데이터베이스 서버로부터 그 변경이 적용됨에 대한 확인confirmation을 수신한 후에, northbound NB_Global 테이블의 sb_cfg를 내려보낸(pushed down) nb_cfg 버전으로 갱신한다. (따라서, CMS 또는 기타 옵저버는 southbound 데이터베이스에 접속 없이 southbound 데이터베이스의 정보 전달 시점을 알 수 있다. )

4. 각 섀시의 ovn-controller 프로세스는 갱신된 nb_cfg를 포함한 갱신된 southbound 데이터베이스를 수신한다. 이 프로세스는 결국 섀시의 Open vSwitch 인스턴스에 설치된 물리 플로우flow를 갱신한다. Open vSwitch로부터 물리 플로우이 갱신되었다는 확답을 수신하면, southbound 데이터베이스의 고유(its own) Chassis 레코드를 갱신한다.

5. ovn-northd는 southbound 데이터베이스에 있는 모든 Chassis 레코드의 nb_cfg 컬럼을 모니터링한다. 이들 모든 레코드의 최소값을 계속 추적하며, 이를 northbound NB_Global 테이블의 hv_cfg 컬럼에 복제한다. (따라서, CMS 또는 기타 옵저버는 언제 모든 하이퍼바이저가 northbound 환경설정을 가지는 지 알 수 있다.)

# 섀시 설정
OVN 배포에서의 각 섀시는 OVN에만 할당된 Open vSwitch 브릿지로 설정되어야 한다. 이는 통합 브릿지(integration bridge)라 불린다. 시스템 시작 스크립트는 ovn-controller를 구동 하기 전에 이 브릿지를 생성한다. 만약 이 브릿지가 ovn-controller 시작 시에 존재하지 않으면, 아래에 제안된 기본 환경설정을 가지고 자동으로 생성될 것이다. 통합 브릿지의 포트는 다음을 포함한다.

* 모든 섀시에서, OVN이 논리 네트웍 연결을 유지하기 위해 사용하는 터널 포트. ovn-controller는 이 터널 포트를 추가, 갱신, 삭제한다.

* 하이퍼바이저에서, 모든 VIF는 논리 네트워크에 연결(attached)된다. 하이퍼바이저 자체, 또는 Open vSwitch 및 하이퍼바이저(IntegrationGuide.rst에 서술됨.) 사이의 통합이 이를 관리한다(이는 OVN의 일부이거나, OVN에서 새로이 가져온 것은 아니다. 이는 OVS를 지원하는 하이퍼바이저가 이미 수행하고 있는 기존 통합 작업이다.).

* 게이트웨이에서, 논리 네트워크 연결을 위한 물리 포트. 시스템 시작 스크립트는 ovn-controller를 시작 하기 전에, 이 포트를 브릿지에 추가한다. 이는 더 복잡한 설정에서, 다른 브릿지로의 패치 포트(patch port)일 수 있다. 물리 포트 대신에 말이다.

다른 포트는 통합 브릿지에 연결되어서는 안된다. 특히, 기저 네트워크에 연결된 물리 포트(논리 네트워크에 연결된 물리 포트인 게이트웨이 포트와는 반대이다.)는 통합 브릿지에 연결되어서는 안된다. 대신 기저 물리 포트는 별도의 Open vSwitch 브릿지에 연결되어야 한다(사실 어떤 브릿지에도 연결될 필요는 없다.).

통합 브릿지는 다음에 서술된 바와 같이 설정되어야 한다. 아래와 같은 설정의 각 의미는 ovs-vswitchd.conf.db(5)에 문서화되어 있다.

- fail-mode=secure
    ovn-controller 시작 전에, 독립된 논리 네트워크 사이의 패킷 전환을 피한다. 자세한 정보는 ovs-vsctl(8)의 Controller Failure Settings 부분을 참고하라.
- other-config:disable-in-band=true
    통합 브릿지를 위한 in-band 제어 플로우를 억제한다. OVN은 원격 컨트롤러 대신 지역 컨트롤러(유닉스 도메인 소켓을 통한)를 사용하기 때문에, 이는 일반적이지는 않다.(추가요망). 그러나 같은 시스템의 다른 브릿지가 in-band 원격 컨트롤러를 갖도록 하는 것이 가능하다. 그리고 이 경우, in-band 제어가 본래 설정된 플로우를 억제한다. 더 자세한 정보는 문서를 참고하라.

통합 브릿지의 관습명은 br-int이다. 그러나 다른 이름도 사용 가능하다.

# 논리 네트워크
논리 네트워크는 물리 네트워크와 같은 개념이다. 그러나, 이는 터널 및 기타 캡슐화를 통해 물리 네트워크로부터 분리된다. 이는 논리 네트워크가 물리 네트워크가 사용중인 것과는 별도의 IP 및 중첩된 다른 주소 공간을 충돌 없이 가질 수 있게 한다. 논리 네트워크 토폴로지는 구동중인 물리 네트워크의 토폴로지와는 관계 없이 할당이 가능하다.

OVN에서의 논리 네트워크 개념은 다음을 포함한다.

* 논리 스위치: 이더넷 스위치의 논리 버전

* 논리 라우터: IP 라우터의 논리 버전. 논리 스위치와 라우터는 복잡한 토폴로지에 접속 가능하다.

* 논리 데이터패스(datapath): 이는 OpenFlow 스위치의 논리 버전이다. 논리 스위치와 라우터는 모두 논리 데이터패스로 구현된다.

* 논리 포트: 이는 논리 스위치와 논리 라우터의 입출력 연결 지점을 표현한다. 논리 포트의 일반적인 형태는 다음과 같다.
  
    * VIF를 나타내는 논리 포트

    * 논리 스위치와 물리 네트워크 간 연결 지점을 나타내는 Localnet 포트. 이들은 통합 브릿지와 별도의 Open vSwitch 브릿지 사이에 OVS 패치 포트로 구현된다. 이는 기저 물리 포트가 연결된다.

    * 논리 스위치와 논리 라우터 간 연결 지점을 나타내는 논리 패치 포트. 어떤 경우에는 논리 라우터간 연결도 가능하다. 각 지점에 논리 패치 포트의 쌍이 존재한다.

    * 논리 스위치와 VIF간의 지역 연결 지점을 표현하는 Localport 포트. 이 포트는 모든 섀시(특정 부분에 속하지 않는.)에 존재하며, 여기서 나오는 트래픽은 터널을 통하지 않는다. localport는 지역을 대상으로 하는 트래픽만 생성할 것으로 기대한다. 보통 수신하는 요청에 반응하는 경우 말이다. 한 예로, OpenStack Neutron 이 localport를 사용하여 모든 하이퍼바이저에 상주하는 VM에게 메타데이터를 제공하는데 사용한다. 메타데이터 프록시 프로세스는 이 모든 호스트에 있는 포트에 연결된다. 같은 네트워크에 있는 모든 VM은 같은 IP/MAC 주소를 사용하여 터널을 통과하지 않고도 도달이 가능하다. 더 자세한 정보는 다음을 확인하라. https://docs.openstack.org/developer/networking-ovn/design/metadata_api.html.

# VIF의 생에 주기
독립되어 표현된 테이블(Table)과 그 스키마는 이해하기가 어렵다. 아래는 그 예이다.

하이퍼바이저의 VIF는 하이퍼바이저에서 직접 구동중인 VM 또는 컨테이너에 연결된 가상 네트워크 인터페이스이다(이는 VM 내에서 구동되는 컨테이너의 인터페이스와는 다르다.).

이 예제의 단계는 OVN과 OVN Northbound 데이터베이스 스키마의 상세를 참조한다. 이 데이터베이스의 전체 내용은 ovn-sb(5)와 ovn-nb(5)를 각각 살펴보라.

1. VIF의 수명 주기는 CMS 관리자가 CMS 사용자 인터페이스나 API를 이용해서 새 VIF를 생성하고, 이를 스위치(OVN를 통해 논리 스위치로 구현된 것.)에 추가할 떄 시작된다. CMS는 그 고유의 환경설정을 갱신한다. 여기에는 해당 VIF와 관련된 고유의, 영구적인 식별자 vif-id와 이더넷 주소 mac이 포함된다.

2. CMS 플러그인은 새 VIF를 포함시키기 위해 Logical_Switch_Port 테이블에 행을 추가함으로써 OVN Northbound 데이터베이스를 갱신한다. 새 행에는, name은 vif-id를, mac은 mac을 뜻하며, switch는 OVN 논리 스위치의 Logical_Switch 레코드를 가리킨다. 또한 다른 열들도 적절히 초기화된다.

3. ovn-northd는 OVN Northbound 데이터베이스 갱신을 수신한다. 이제 대응되는 갱신을 OVN Southbound 데이터베이스에 수행한다. 새 포트를 반영하기 위해 OVN Southbound 데이터베이스 Logical_Flow 테이블을 추가함으로써 말이다. 예를 들어, 새 포트의 MAC 주소로 향하는 패킷을 인지하기 위핸 플로우(flow) 추가 및 새 포트를 포함하는 브로드캐스트 및 멀티캐스트 패킷을 전달하기 위한 플로우 갱신이 그것이다. 또한 Binding 테이블의 레코드를 생성하고, chassis를 식별하는 열을 제외한 모든 열을 populate한다.

4. 모든 하이퍼바이저에서, ovn-controller는 이전 단계에서 ovn-northd가 수행한 Logical_Flow 테이블의 갱신을 수신한다. 해당 VIF를 소유하는 VM이 전원이 꺼져 있는 한, ovn-controller는 많은 일을 수행할 수 없다. 예를 들어, VIF가 실제로는 어디에도 존재하지 않기 떄문에, 해당 VIF에서 패킷을 보내거나 수신하는 것을 할당할 수 없다.

5. 언젠가 사용자는 해당 VIF를 갖는 VM의 전원을 켤 것이다. 켜진 VM이 존재하는 하이퍼바이저에서, 하이퍼바이저와 Open vSwitch(IntegrationGuide.rst에 서술됨) 사이의 통합은 해당 VIF를 OVN 통합 브릿지에 추가하고, 해당 인터페이스가 새 VIF의 인스턴스화라는 것을 가리키기 위해 vif-id를 external_ids:iface-id에 저장한다(이 코드는 OVN에서 새로 구현된 것이 아니다. 이는 OVS를 지원하는 하이퍼바이저가 이미 수행하고 있는 통합 작업이다.).

6. 켜진 VM이 있는 하이퍼바이저에서, ovn-controller는 새 인터페이스의 external_ids:iface-id를 인지한다. OVN Southbound DB에서, 이는 external_ids:iface-id로부터의 논리 포트를 하이퍼바이저에 연결하는 열인 Binding 테이블의 chassis 열을 갱신한다. 그 후, ovn-controller는 지역 하이퍼바이저의 OpenFlow 테이블을 갱신하여 해당 VIF를 출입하는 패킷을 적절히 처리하게 한다.

7. OpenStack을 포함하는 몇몇 CMS 시스템에서는 네트워크가 완전히 준비된 경우에만 VM을 구동한다. 이를 지원하기 위해, ovn-northd는 Binding 테이블에 있는 열을 위해 갱신된 chassis 열을 인지하고, 해당 VIF가 이제 사용 가능하다는 것을 가리키기 위해 OVN Northbound 데이터베이스의 Logical_Switch_Port 테이블의 up 열을 갱신하도록 한다. 이 기능을 사용하는 CMS는 VM의 실행을 허용한다.

8. VIF가 상주하고 있지 않은 모든 하이퍼바이저에서, ovn-controller는 Binding 테이블에 poulated된 열을 인지한다. 이는 ovn-controller에 논리 포트의 물리 위치를 제공한다. 따라서, 각 인스턴스는 그 스위치(OVN DB Logical_Flow 테이블의 논리 데이터패스 플로우를 기반으로)의 OpenFlow 테이블을 갱신하여 해당 VIF에 출입하는 패킷은 터널을 통해 적절히 처리된다.

9. 끝으로, 해당 VIF를 갖는 VM의 전원을 끊을 것이다. 꺼진 VM이 있는 하이퍼바이저에서, VIF는 OVN 통합 브릿지에서 삭제된다.

10. 전원이 꺼진 VM을 갖는 하이퍼바이저에서, ovn-controller는 VIF가 삭제되었음을 인지한다. 따라서, Binding 테이블의 논리 포트를 나타내는 Chassis 열의 내용을 삭제한다.

11. 모든 하이퍼바이저에서, ovn-controller는 Binding 테이블의 논리 포트를 위한 행에서 빈 Chassis 열을 인지한다. 이는 ovn-controller가 더 이상 해당 논리 포트의 물리적 위치를 알지 못한다는 것이다. 따라서, 각 인스턴스는 이를 반영하기 위해 그 OpenFlow 테이블을 갱신한다.

12. 결국 VIF(또는 그 VM전체)가 더 이상 누구에게도 필요하지 않은 경우, 관리자는 CMS 사용자 인터페이스 또는 API를 사용해서 VIF를 삭제한다. CMS는 그 환경설정을 갱신한다.

13. CMS 플러그인은 OVN Northbound 데이터베이스에서 VIF를 삭제한다. Logical_Switch_Port 테이블의 열을 삭제함으로써 말이다.

14. ovn-northd는 OVN Northbound 갱신을 수신하고, OVN Southbound 데이터베이스를 그에 맞게 갱신한다. OVN Southbound 데이터베이스에서 삭제된 VIF와 관련된 Logical_Flow 테이블과 Binding 테이블에서 열을 삭제 또는 갱신한다.

15. 모든 하이퍼바이저에서, ovn-controller는 이전 단계에서 ovn-northd가 수행한 Logical_Flow 테이블의 갱신을 수신한다. ovn-controller는 해당 갱신을 반영하기 위해 OpenFlow 테이블을 갱신한다. 할 수 있는게 많진 않지만, 이전 단계에서 Binding 테이블로부터 삭제가 되었을 때 이미 해당 VIF는 도달할 수 없게 되었을 것이다.

# VM 내의 컨테이너 인터페이스의 수명 주기
OVN은 각 하이퍼바이저에서 OVN_NB 데이터베이스에 작성된 정보를 OpenFlow 플로우로 전환하는 방식으로 가상 네트워크 추상화를 제공한다. 다중 테넌트(tenant)를 위한 가상 네트워크 보안은 OVN 컨트롤러가 Open vSwitch에서 플로우를 수정할 수 있는 유일한 요소인 경우에만 제공된다. Open vSwitch 통합 브릿지가 하이퍼바이저에 상주할 때, VM 내에서 동작하는 테넌트 작업은 Open vSwitch 플로우에는 어떠한 변경도 가할 수 없다고 생각하는 것이 합리적이다.

인프라스트럭처로부터 컨테이너에 있는 애플리케이션이 Open vSwitch 플로우를 고장내거나 수정하지 않는다는 신뢰를 제공한다면, 컨테이너는 하이퍼바이저에서 실행이 가능하다. 이는 또한, 컨테이너가 VM 내부에서 실행되고 OVN 컨트롤러가 추가한 플로우를 갖는 Open vSwitch 통합 브릿지가 같은 VM에 있을 때에도 해당된다. 두 경우 모두, 작업흐름은 이전 절("VIF의 생명주기")에서 설명한 것과 같다.

이 절에서는 컨테이너가 VM내에서 생성되고, Open vSwitch 통합 브릿지가 하이퍼바이저 내에 상주하는 경우 컨테이너 인터페이스(CIF)의 생에 주기에 대해 다룬다. 이 경우, 컨테이너 애플리케이션이 오동작한다 하더라도, 다른 테넌트는 영향 받지 않을 것이다. 왜냐하면, VM 내에 구동중인 컨테이너는 Open vSwitch 통합 브릿지의 플로우를 수정할 수 없기 때문이다.

VM 내에 여러 컨테이너가 생성되면, 대응되는 여러 CIF가 생긴다. 이들 CIF에 관련된 네트워크 트래픽은 OVN이 가상 네트워크 추상화를 지원하기 위해 하이퍼바이저에 실행중인 Open vSwitch 통합 브릿지에 도달해야 한다. OVN은 또한 서로 다른 CIF에서 도달하는 네트워크 트래픽을 구분할 수 있어야 한다. CIF의 네트워크 트래픽을 구분짓는데는 두 가지 방법이 존재한다.

하나는 모든 CIF에 VIF를 제공하는 것이다(1:1 모델). 이는 하이퍼바이저에 많은 네트워크 장치가 존재할 수 있다는 것을 뜻한다. 모든 CIF의 관리를 위해 필요한 추가적인 CPU 주기가 필요하므로 OVS를 느리게할 수 있다. 또한, VM 내에 컨테이너를 생성 요소들 또한 하이퍼버이저의 대응되는 VIF를 생성할 수 있어야 한다는 것을 말한다.

둘째는 모든 CIF에 하나의 VIF를 제공하는 것이다(1:다 모델). OVN은 모든 패킷에 쓰여진 태그(tag)를 통해 서로 다른 CIF로 부터 오는 네트워크 트래픽을 구분할 수 있다. OVN은 이 기재를 사용하고, 태그 기재를 위해서는 VLAN을 사용한다.

1. CIF의 수명 주기는 VM 내에 컨테이너가 생성됨으로써 시작된다. 이는 VM을 생성한 같은 CMS로부터 또는 VM을 생성한 CMS와는 다른 컨테이너 Orchestration 시스템에 의해서 일 수 있다. 그것이 무엇이든, 컨테이너 인터페이스의 네트워크 트래픽이 향할 것이라 생각되는 VM의 네트워크 인터페이스에 대응되는 vif-id를 알아야 한다. 컨테이너 인터페이스를 생성하는 요소 또한 VM 내의 미사용 VLAN을 선택할 수 있어야 한다.

2. 컨테이너를 생성한 요소()는 OVN Northbound 데이터베이스를 갱신하여, 새 CIF를 포함하도록 해야 한다. Logical_Switch_port 테이블에 행을 추가한다. 새 행에는, 고유 식별자로써 name을, CIF의 네트워크 트래픽이 지나쳐야 할 VM의 vif-id로써 parent_name을, CIF의 네트워크 트래픽을 식별하기 위한 VLAN 태그로써 tag를 갖는다.

3. ovn-northd는 OVN Northbound 데이터베이스 갱신을 수신한다. 다음으로, OVN Southbound 데이터베이스에 대응되는 바를 갱신한다. OVN Southbound 데이터베이스의 Logical_Flow 테이블에 새 포트를 반영하는 행을 추가한다. 또한 Binding 테이블에 새 행을 생성하고 chassis를 식별하는 열을 제외한 모든 열을 populate한다.

4. 모든 하이퍼바이저에서, ov-controller는 Binding 테이블의 변경을 주시(subscribe)하고 있다. Binding 테이블의 parent_port 열에 값을 포함하는 새 행이 ovn-northd로부터 생성되면, external_ids:iface-id의 vif-id 값과 같은 값을 갖는 OVN 통합 브릿지를 갖는 하이퍼바이저의 ovn-controller는 지역 하이퍼바이저의 OpenFlow 테이블 값을 갱신한다. 따라서, VIF를 출입하는 특정 VLAN tag가 달린 패킷은 적절히 처리된다. 그후, 물리 위치를 반영하기 위해 Binding의 chassis 열을 갱신한다.

5. 기저 네트워크가 준비되고 나면 컨테이너 내부에는 하나의 애플리케이션만 시작 가능하다. 이를 지원하기 위해, ovn-northd는 Binding 테이블의 chassis 열의 갱신을 인지하고, CIF가 이제 활성화되었다는 것을 가리키기 위해 OVN Northbound 데이터베이스의 Logical_Switch_Port 테이블의 up 열을 갱신한다. 컨테이너 애플리케이션을 시작할 책임이 있는 요소는 해당 값을 질의한 후 애플리케이션을 시작한다.

6. 종국에는, 컨테이너를 생성하고 시작한 요소가 이들을 종료한다. 이 요소는 CMD(또는 직접)를 통해 Logical_Switch_Port 테이블의 행을 삭제한다.

7. ovn-northd는 OVN Northbound 갱신을 수신하고, OVN Southbound 데이터베이스를 그에 따라 갱신한다. 삭제된 CIF와 관련된 OVN Southbound 데이터베이스의 Logical_Flow 테이블의 행을 제거 또는 갱신한다. 또한 CIF를 위한 Binding 테이블의 열을 삭제한다.

8. 모든 하이퍼바이저에서, ovn-controller는 이전 단계에서 ovn-northd가 수행한 Logical_Flow 테이블의 갱신을 수신한다. ovn-controller는 해당 갱신을 반영하기 위해 OpenFlow 테이블을 갱신한다.

# 패킷의 구조적 물리적 수명 주기

이 절에서는 패킷이 OVN을 통해 하나의 가상 머신 또는 컨테이너에서 다른 곳으로 여행하는 과정을 서술한다. 이 부분에서는 패키의 물리적 처리에 집중한다. 패킷의 논리적 수명 주기의 서술의 경우 ovn-sb(5)의 Logical_Flow 테이블을 참조하라.

이 절에서는 아래와 같이 요약할 수 있는 여러 데이터 및 메타데이터 필드를 언급한다.

  - 터널 키(tunnel key)
    OVN이 Geneve 또는 기타 터널을 통해 패킷을 캡슐화하면, 수신하는 OVN 인스턴스가 그를 적절히 처리할 수 있도록 하기 위해 추가 데이터를 붙인다. 이는 특정 캡슐화에 따라 서로 다른 형태를 가지지만, 이 터널 키 가 가리키는 각 경우에 따르면 된다. 자세한 사항은 아래의 터널 캡슐화(Tunnel Encapsulations)를 보라.

  - 논리 데이터패스 필드
    이 필드는 처리되어야 할 패킷이 통과 하는 논리적 데이터패스를 나타낸다. OVN은 OpenFlow 1.1+에서 간단히 metadata(헷갈린다)라 부르는 이 필드를 논리 데이터패스를 저장하기 위해 사용한다(이 필드는 터널 키의 일부로 터널에 걸쳐 전달된다.).

  - 논리 입력 포트 필드
    이 필드는 논리 데이터패스에 진입한 패킷이 어디에서 온 것인지를 가리키는 논리 포트를 나타낸다. OVN은 Open vSwitch 확장 레지스터 번호 14(확인요망)에 이를 저장한다.

    Geneve와 STT 터널은 이 필드를 터널 키의 일부로 전달한다. 비록 VXLAN 터널은 명시적으로 논리 입력 포트를 전달하지는 않지만, OVN은 단일 논리 포트만으로 구성된 OVN의 관점으로 VXLAN을 사용하여 게이트웨이와 통신한다. 따라서 OVN은 논리 입력 포트 필드를 이 값으로 설정할 수 있다. (OVN can set the logical input port
                     field to this one on ingress to the OVN logical
                     pipeline.))
  - 논리 출력 포트 필드
    이 필드는 논리 데이터패스를 떠나는 패킷이 어느 논리 포트로 나갈지를 가리킨다. 이는 논리 인입 파이프라인의 시작에는 0으로 초기화된다. OVN은 Open vSwitch 확장 레지스터 번호 15에 이를 저장한다.

    Geneve와 STT 터널은 이 필드를 터널 키의 일부로 전달한다. VXLAN 터널은 이 논리 출력 포트 필드를 전송하지 않는다. VXLAN 터널이 터널 키에 논리 출력 포트 필드를 전달하지 않기 때문에, 패킷이 OVN 하이퍼바이저에 의해 VXLAN 터널로부터 수신되면, 패킷은 출력 포트를 결정하기 위해 테이블 8로 재전송된다. 패킷이 테이블 32에 도달하면, 이 패킷에서 MLF_RCV_FROM_VXLAN 플래그를 점검하여 지역 전송을 위해 테이블 33으로 재전송된다. 이 플래그는 VXLAN 터널에서 패킷이 도착할 때 설정된다.

  - 논리 포트를 위한 conntrack 존 필드
    이 필드는 논리 포트를 위한 연결 추적 존을 나타낸다. 이 값은 지역적인 의미만 가지고 있고, 섀시 간에는 의미가 없다. 이 값은 논리 인입 파이프라인의 시작에 0으로 초기화된다. OVN은 이를 Open vSwitch 확장 레지스터 번호 13에 저장한다.

  - 라우터를 위한 conntrack 존 필드
    이 필드는 라우터를 위한 연결 추적 존을 나타낸다. 이 값은 지역적인 의미만 가지고 있고, 섀시 간에는 의미가 없다. OVN은 DNAT을 위한 존 정보를 Open vSwitch 확장 레지스터 번호 11에 저장하고, SNat를 위한 존 정보는 Open vSwitch 확장 레지스터 번호 12에 저장한다.

  - 논리 플로우 플래그 (추후확인)
    논리 플래그는 테이블 간 컨텍스트 유지를 다루기 위한 것이다. 어떤 규칙을 뒤따르는 테이블에 매치하도록 할 건지 결정하기 위해서 말이다. OVN은 논리 플래그를 Open vSwitch 확장 레지스터 번호 10에 저장한다.

  - VLAN ID
    VLAN ID는 OVN과 VM에 있는 컨테이너 간 인터페이스로써 사용된다(더 자세한 정보는 VM 내의 컨테이너 인터페이스의 수명 주기를 보라.).

초기에, 인입 하이퍼바이저의 VM 또는 컨테이너는 OVN 통합 브릿지에 연결된 포트로 패킷을 보낸다. 따라서,

  1. OpenFlow table 0은 물리-가상 전환을 수행한다. 이는 패킷의 진입 포트를 match시킨다. 그 액션(action)은 패킷에 논리 메타데이터라는 주석을 단다. 패킷이 지나다니는 논리 데이터패스 식별을 위한 논리 데이터패스 필드 및 진입 포트를 식별하기 위한 논리 입력 포트 필드를 설정함으로써 말이다. 그러고 나서, 논리 진입 파이프라인에 들어가기 위해 테이블 8로 재전송한다.

  VM내의 중첩 컨테이너로부터 기인하는 패킷은 조금 다른 방식으로 다뤄진다. 해당 컨테이너는 VIF 특정 VLAN ID를 기반으로 구분할 수 있다. 따라서, 물리-논리 변환 플로우는 추가적으로 VLAN ID와 match되며, 액션(action)은 VLAN 헤더를 제거한다. 이 단계 다음에는, OVN은 컨테이너에서 온 패킷을 다른 패킷과 마찬가지로 다룬다.

  테이블 0은 또한 다른 섀시로부터 도달하는 패킷을 처리한다. 이는 진입 포트(즉 터널)에서 오는 다른 패킷과 구분한다. OVN 파이프라인에 막 들어오는 패킷의 경우, 액션은 이러한 패킷에 논리 데이터패스와 논리 진입 포트 메타데이터를 붙인다(annotate). 추가로, 액션은 논리 출력 포트 필드를 설정한다. 이는 논리 출력 포트가 알려진 이후에 OVN 터널링이 발생하기 때문에 사용 가능하다. 이러한 정보의 세 조각은 터널 캡슐화 메타데이터로부터 얻을 수 있다(인코딩 상세는 터널 캡슐화를 보라.). 그러고 나서, 액션은 논리 진출 파이프라인에 들어가기 위해 테이블 33에 재전송한다.

  2. OpenFlow 테이블 8에서 31은 OVN Southband 데이터베이스의 Logical_Flow 테이블로부터 논리 진입 파이프라인을 실행한다. 이 테이블은 논리 포트 및 논리 데이터패스와 같은 논리적인 개념으로 모두 표현된다. ovn-controller의 작업 중 큰 부분은 이를 대응되는 OpenFlow로 변환하는 것이다(특히 이는 표 번호를 변환한다. Logical_Flow 테이블 0에서 23은 OpenFlow 테이블 8에서 31로 변환된다.).

  각 논리 플로우는 하나 이상의 OpenFlow 흐름에 매핑된다. 실제 패킷은 보통 이 중 하나에만 매치(match)된다. 비록 어떤 경우에는 하나 이상의 흐름에 매칭이 가능하지만 말이다(이것이 큰 문제는 아닌것이, 이들 모두는 같은 액션을 갖기 때문이다.). ovn-controller는 논리 플로의 UUID의 첫 32비트를 OpenFlow 흐름 또는 흐름들을 위한 쿠키(cookie)로 사용한다(이들은 고유할 필요는 없다. 논리 흐름의 UUID의 첫 32비트는 고유할 필요가 없다.).

  몇몇 논리 플로우는 Open vSwitch으 ㅣconjuctive match 확장(ovs-fields(7)을 보라.)에 매핑될 수 있다. conjuction 액션을 갖는 플로우는 OpenFlow 쿠키 0을 사용한다. 이들이 다수의 논리 흐름과 대응되기 때문이다. 공동(conjuctive) 매칭을 위한 OpenFlow 흐름에는 conj_id의 매칭도 포함된다.

  몇몇 논리 플로우는 주어진 하이퍼바이저의 OpenFlow 테이블에 표현되지 않을 수 있다. 만약 하이퍼바이저에서 이들이 사용되지 않는다면 말이다. 예를 들어, 논리 스위치에 있는 VIF가 주어진 하이퍼바이저에 없고, 논리 스위치가 해당 하이퍼바이저에서 접근이 불가능하다면(예, over a series of hops through logical switches and routers starting from a VIF on the hypervisor), 해당 논리 플로우는 그곳에 표현되지 않을 것이다.

  대부분의 OVN 액션은 OpenFlow에 제법 명확한 구현을 갖는다. 예를 들어 next는 resubmit으로 구현되었으며, field = constant는 set_field로 구현되었다. 몇가지가 더 있는데, 다음을 보자.

  output:
    테이블 32로 재전송되는 패킷에 의해 구현된다. 만약 파이프라인이 하나 이상의 output 액션을 수행한다면, 각각은 독립적으로 테이블 32로 재전송된다. 이는 다중 포트로 다수의 패킷 복사본을 전송하는데 사용될 수 있다( 만약 패킷이 output 액션에서 수정되지 않았고, 복사본 중 일부가 같은 하이퍼바이저로 간다면, 논리 멀티캐스트 출력 포트를 사용하는 것이 하이퍼바이저 간 대역폭을 아끼는 길일 것이다.).

  get_arp(P, A):
  get_nd(P, A):
    인자(arguments)를 OpenFlow 필드에 저장하는 걸로 구현되었다. 그러고 나서, 테이블 66으로 재전송된다. 이 테이블은 ovn-controller가 OVN Southband 데이터베이스에 있는 MAC_Binding 테이블로 부터 생성된 플로우를 populate 한다. 이들이 테이블 66에 매칭되면, 그 액션은 이더넷 목적지 주소 필드에 연결된 MAC을 저장한다.

    (OpenFlow 액션은 인자를 위해 사용된 OpenFlow 필드를 저장하고 복구(restore)한다. 따라서 OVN 액션은 이러한 일시적인 사용을 인지할 필요가 없다.)

  put_arp(P, A, E);
  put_nd(P, A, E);
    인자를 OpenFlow 필드에 저장하기 위해 구현되었다. 그러고 나서 패킷을 ovn-controller로 보낸다. ovn-controller는 MAC_Binding 테이블을 갱신한다.

    (OpenFlow 액션은 인자를 위해 사용된 OpenFlow 필드를 저장하고 복구(restore)한다. 따라서 OVN 액션은 이러한 일시적인 사용을 인지할 필요가 없다.)

  3. OpenFlow 테이블 32부터 47은 논리 진입 파이프라인에서 output 액션을 구현한다. 특히 테이블 32는 원격 하이퍼바이저로의 패킷을 처리, 테이블 33은 지역 하이퍼바이저로의 패킷을 처리하며, 테이블 34는 논리 진입 및 진출 포트가 같아서 버려져야 하는 패킷이 있는지 점검한다.

  논리 패치 포트는 특별한 경우이다. 논리 패치 포트는 물리적 위치를 갖지 않으며, 모든 하이퍼바이저에 사실상 상주한다. 따라서, 지역 하이퍼바이저의 포트로의 출력을 위한 플로우를 테이블 33은 자연히 유니캐스트 논리 패치 포트로의 출력 또한 구현한다. 그러나, 논리 멀티캐스트 그룹의 일부인 논리 패치 포트로 같은 로직을 적용하는 것은, 패킷 복제를 야기한다. 멀티캐스트 그룹에서 논리 포트를 갖는 각 하이퍼바이저 또한 논리 패치 포트로 패킷을 출력할 것이기 때문이다. 따라서, 멀티캐스트 그룹은 테이블 32에 논리 패치 포트로의 출력을 구현한다.

  테이블 32의 각 플로우는 원격 하이퍼바이저에 논리 포트를 포함하는 유니캐스트 또는 멀티캐스트 논리 포트를 위해 논리 출력 포트를 매칭한다. 각 플로우의 액션은 매칭된 포트로의 패킷 전송을 구현한다. 원격 하이퍼바이저의 유니캐스트 논리 출력 포트의 경우, 액션은 터널 키를 올바를 값으로 설정한 후, 올바를 하이퍼바이저로 향하는 터널 포트로 패킷을 전송한다(원격 하이퍼바이저가 패킷을 받으면, 테이블 0은 터널링된 패킷을 인지할 것이고, 이를 테이블 33으로 보낼 것이다.). 멀티캐스트 논리 출력 포트의 경우, 액션은 패킷의 한 복사본을 각 원격 하이퍼바이저로 보낸다. 유니캐스트 목적지의 경우와 같은 방식으로 말이다. 멀티캐스트 그룹에 지역 하이퍼바이저 포트(들)이 포함된 경우, 그 액션은 테이블 33으로 재전송한다. 테이블 32는 또한 다음을 포함한다.
    * MLF_RCV_FROM_VXLAN 플래그를 기반으로, VXLAN 터널로 부터 수신된 패킷을 매칭하기 위한 높은 우선순위의 규칙. 그리고 이들 패킷을 지역 전달을 위해 테이블 33으로 재전송된다. VXLAN 터널로 부터 수신된 패킷은 여기에 도달한다. 터널 키에 논리 출력 포트 필드가 없기 때문이다. 그리고 이들 패킷은 출력 포트를 결정하기 위해 테이블 8로 재전송되어야 한다.
    
    * 논리 입력 포트를 기반으로 하는 localport 형태의 포트로부터 수신된 패킷을 매칭하기 위한 높은 우선순위 규칙. 그리고 이들 패킷은 지역 전달을 위해 테이블 33으로 재전송된다. localport 형태의 포트는 모든 하이퍼바이저에 존재하고, 정의에 따라 그들의 트래픽은 터널을 통해 나가서는 안된다.

    * MLF_LOCAL_ONLY 논리 플로우 플래그가 설정되었고 그 목적지가 멀티캐스트 주소인 패킷을 매칭하기 위한 높은 우선순위 규칙. 이 플래그는 패킷이 원격 하이퍼바이저로는 전달되어서는 안된다는 것을 가리킨다. 멀티캐스트 목적지에 원격 하이퍼바이저의 포트가 포함되어 있다고 하더라도 말이다. 이 플래그는 ovn-controller가 멀티캐스트 패킷의 전송자일 경우에만 사용된다. 각 ovn-controller 인스턴스가 이들 패킷의 전송자이기 때문에, 해당 패킷은 지역 포트로만 전달될 필요가 있다.

    * 매칭이 되지 않은 경우 테이블 33으로 재전송되는 폴백(fallback) 플로우

  테이블 33에 있는 플로우는 테이블 32에 있는 것들과 유사하지만, 원격 보다는 지역적으로 존재하는 논리 포트를 위한 것이다. 지역 하이퍼바이저의 유니캐스트 논리 출력 포트의 경우, 액션은 단지 테이블 34로 재전송할 뿐이다. 지역 하이퍼바이저의 하나 이상의 논리 포트를 포함하는 멀티캐스트 출력 포트의 경우, 논리 포트 P와 같은 각각을 위해 액션은 논리 출력 포트를 P로 변경한다. 그러고 나서 테이블 34로 재전송한다.

  localnet 포트가 데이터패스에 존재할 때, 원격 포트가 localnet 포트로 스위칭(switching)을 통해 연결되는 특별한 경우가 있다. 이 경우, 원격 포트에 도달하기 위해 플로우를 테이블 32로 추가하는 대신, 논리 출력포트를 localnet 포트로 스위칭하기 위해 플로우를 테이블 33에 추가한다. 그러고 나서 마치 지역 하이퍼바이저에 논리 포트로 유니캐스트된 것 처럼 테이블 33으로 재전송한다.

  테이블 34는 논리 입력 및 출력 포트가 같고 MLF_ALLOW_LOOPBACK 플래그가 설정되지 않은 패킷을 매칭하고 버린다. 이는 다른 패킷을 테이블 40에 재전송한다.

  4. OpenFlow 테이블 40부터 63은 OVN Southbound 데이터베이스의 Logical_Flow 테이블로부터 논리 진출 파이프라인을 실행한다. 진출 파이프라인은 패킷이 전달되기 전에 마지막 검증 절차를 수행한다. 끝으로, 이는 ovn-controller가 테이블 64로 재전송함으로써 구현된 output 액션을 수행할 것이다. output을 절대 수행하지 않는 파이프라인을 위한 패킷은 사실 버려진다(물리 네트워크의 터널을 통해 재전송되었다 해도 말이다.).

  진출 파이프라인은 논리 출력 포트를 변경할 수 없고, 추가적인 터널링을 일으키지 않는다.

  5. 테이블 64는 MLF_ALLOW_LOOPBACK이 설정된 경우 OpenFlow 루프백을 우회한다. 논리 루프백은 테이블 34에서 처리되지만, OpenFlow는 기본적으로 OpenFlow 진입 포트로의 루프백을 막는다. 게다가, MLF_ALLOW_LOOPBACK이 설정되면, OpenFlow 테이블 64는 OpenFlow 진입 포트를 저장하고, 이를 0으로 설정한 후, 논리-물리 변환을 위해 테이블 65로 재전송한다. 그러고 나서, OpenFlow 진입 포트를 복구하고, OpenFlow 루프백 금지를 비활성화 한다. MLF_ALLOW_LOOPBACK이 설정 해제되면, 테이블 64 플로우는 단순히 테이블 65로 재전송된다.

  6. OpenFlow 테이블 65는 논리-물리 변환을 수행한다. 이는 테이블 0의 정반대이다. 이는 패킷의 논리 진출 포트를 매칭한다. 그 액션은 논리 포트를 표현하는 OVN 통합 브릿지로 연결된 포트로 패킷을 출력한다. 만약 논리 진출 포트가 VM내의 중첩된 컨테이너라면, 패킷을 전송하기 전에, 액션은 적절한 VLAN ID를 포함하는 VLAN 헤더를 얹는다.
   
# 논리 라우터 및 논리 패치 포트
보통 논리 라우터와 논리 패치 포트는 물리 위치를 갖지 않고, 사실 모든 하이퍼바이저에 상주한다. 이는 VM(및 VIF)가 연결된 논리 라우터와 그 논리 라우터 이면의 논리 스위치 사이의 논리 패치 포트에 해당된다.

서로 다른 서브넷에 존재하는 한 가상 머신 또는 컨테이너에서 다른 VM 또는 컨테이너로 전송된 패킷을 생각해 보자. 패킷은 이전 절(패킷의 구조적 물리적 수명 주기)에서 설명한 테이블 0에서 65로 이동할 것이다. 송산자가 연결된 논리 스위치를 나타내는 논리 데이터패스를 통해 말이다. 테이블 37에서, 해당 패킷은 같은 하이퍼바이저의 테이블 38로 재전송하는 폴백(fallback) 플로우를 사용할 것이다. 이 경우, 테이블 0에서 테이블 65의 모든 절차는 송신자가 있는 하이퍼바이저에서 발생한다.

패킷이 테이블 65에 도달하면, 논리 진입 포트는 논리 패치 포트이다. ovn-controller는 진입 파이프라인의 첫 OpenFlow 플로우 테이블 에 직접 복제 및 재전송하고, 논리 패치 포트 쌍으로 논리 진입 포트를 설정하고, 논리 패치 포트 쌍의 논리 데이터패스(이는 논리 라우터를 나타낸다.)를 사용함으로써 논리 패치로의 출력을 구현한다.

테이블 8에서 65을 다시 거치기 위해 진입 파이프라인에 재진입하는 패킷은 이번에는 논리 라우터를 나타내는 논리 데이터패스를 사용한다. 이 절차는 이전 절(패킷의 구조적 물리적 수명 주기)에서 설명한 대로 진행된다. 패킷이 테이블 65에 도달하면, 논리 진출 포트는 한 번 더 논리 패치 포트에 있을 것이다. 위에 설명한 방식 대로, 이 논리 패치 포트는 OpenFlow 테이블 8에서 65까지 패킷 재전송을 야기할 것이다. 이번에는 목적지 VM 또는 컨테이너에 연결된 논리 스위치를 나타내는 논리 데이터패스를 사용한다.

해당 패킷은 세번째이자 마지막으로 테이블 8에서 65까지 거친다. 목적지 VM 또는 컨테이너가 원격 하이퍼바이저에 존재한다면, 테이블 37은 송신자의 하이버파이저에서 원격 하이퍼바이저로 터널 포트을 통해 패킷을 전송한다. 끝으로, 테이블 65는 목적지 VM 또는 컨테이너에 직접 패킷을 출력할 것이다.

다음에 나오는 절은 두 예외를 서술한다. 이들은 논리 라우터/논리 패치 포트가 물리 위치에 연결된 경우이다.

## 게이트웨이 라우터

게이트웨이 라우터는 물리 위치에 묶인 논리 라우터이다. 여기에는 논리 라우터의 모든 논리 패치 포트를 포함한 논리 스위치의 논리 패치 포트 쌍 모두가 포함되어 있다. OVN Southbound  데이터베이스에서, 이 논리 패치 포트를 위한 Port_Binding 엔트리는 l3gateway 타입을 patch 대신 사용한다. 섀시에 묶인 이들 논리 패치 포트를 구별하기 위해서 말이다.

하이퍼바이저가 논리 스위치를 나타내는 논리 데이터패스에서 패킷을 처리하고, 논리 진출 포트가 게이트웨이 라우터에 연결을 나타내는 l3gateway 포트라면, 패킷은 게이트웨이 라우터가 상주하는 섀시로 터널 포트를 통해 패킷을 전달하는 테이블 37에 플로우를 매칭시킬 것이다. 테이블 37의 이러한 절차는 VIF에서와 같은 방식으로 이뤄진다.

## 분산 게이트웨이 포트





# 논리 라우터에 연결된 다중 로컬넷(localnet) 논리 스위치

# VTEP 게이트웨이의 수명 주기

# 외부 논리 포트를 위한 네이티브 OVN 서비스